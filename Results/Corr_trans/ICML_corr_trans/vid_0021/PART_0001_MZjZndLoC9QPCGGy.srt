1
00:00:01,210 --> 00:00:06,476
In reinforcement and imitation learning, the agent's policy induces a discounted state distribution and state-action distribution.

2
00:00:07,011 --> 00:00:09,672
They are of central importance, appearing all across the literature.

3
00:00:09,692 --> 00:00:14,694
They form the basis of policy gradient methods in RL and are core to the distribution matching formulation of imitation learning.

4
00:00:15,254 --> 00:00:21,096
They are also foundational to other applications like curiosity-based exploration, constrained RL, batch RL, and convex RL.

5
00:00:21,616 --> 00:00:28,739
Despite this ubiquity across the literature, the distributions are mostly discussed indirectly and theoretically, rather than being modeled explicitly.

6
00:00:29,219 --> 00:00:35,621
And so this would concentrate on modeling them explicitly with modern density estimators, specifically normalizing flows, focusing on imitation learning.

7
00:00:36,806 --> 00:00:43,188
The simplest approach to imitation is behavioral cloning, which reformulates supervised regression and maximum likelihood on given expert state-action pairs.

8
00:00:43,909 --> 00:00:49,971
Modern approaches to imitation instead attempt to match the agent's state action distribution with the experts by minimizing some f divergence.

