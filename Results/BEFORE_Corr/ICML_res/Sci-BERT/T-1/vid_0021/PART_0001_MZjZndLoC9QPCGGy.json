{
    "MatchedRegions": [
        {
            "start_time": "00:00:01,210",
            "end_time": "00:00:06,476",
            "transcript": "In reinforcement and imitation learning, the agent's policy induces a discounted state distribution and state-action distribution.",
            "MatchedRegion": [
                {
                    "Text": "In RL & IL, the agent's policy induces a state distribution d(s) and state-action distribution P(s,a) = n(as).dz(s) .",
                    "BoundingBox": {
                        "Width": 0.8002861142158508,
                        "Height": 0.06716174632310867,
                        "Left": 0.08649010211229324,
                        "Top": 0.2614990472793579
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "43e58ab5-41bc-44cc-89af-8b3ae8109207",
                    "Score": 0.8185226321220398
                },
                {
                    "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
                    "BoundingBox": {
                        "Width": 0.8082616329193115,
                        "Height": 0.06645342707633972,
                        "Left": 0.08673549443483353,
                        "Top": 0.6041567921638489
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "50255d04-07c8-4a5d-8823-990b60678720",
                    "Score": 0.8130738735198975
                },
                {
                    "Text": "This work concentrates on modeling them explicitly with normalizing flows, focusing on imitation learning.",
                    "BoundingBox": {
                        "Width": 0.7311545014381409,
                        "Height": 0.027620358392596245,
                        "Left": 0.12032857537269592,
                        "Top": 0.6909106373786926
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "5d85eff4-4325-46fe-bf5c-9cd97149d0e8",
                    "Score": 0.8247489929199219
                }
            ]
        },
        {
            "start_time": "00:00:07,011",
            "end_time": "00:00:09,672",
            "transcript": "They are of central importance, appearing all across the literature.",
            "MatchedRegion": [
                {
                    "Text": "They are of central importance, appearing all across the literature:",
                    "BoundingBox": {
                        "Width": 0.5164313316345215,
                        "Height": 0.03070523776113987,
                        "Left": 0.08655297011137009,
                        "Top": 0.34929364919662476
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "a91c0711-79e1-4748-afc4-5c2a6ecbc1b9",
                    "Score": 0.8861502408981323
                },
                {
                    "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
                    "BoundingBox": {
                        "Width": 0.8082616329193115,
                        "Height": 0.06645342707633972,
                        "Left": 0.08673549443483353,
                        "Top": 0.6041567921638489
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "50255d04-07c8-4a5d-8823-990b60678720",
                    "Score": 0.8130738735198975
                }
            ]
        },
        {
            "start_time": "00:00:09,692",
            "end_time": "00:00:14,694",
            "transcript": "They form the basis of policy gradient methods in RL and are core to the distribution matching formulation of imitation learning.",
            "MatchedRegion": [
                {
                    "Text": "This work concentrates on modeling them explicitly with normalizing flows, focusing on imitation learning.",
                    "BoundingBox": {
                        "Width": 0.7311545014381409,
                        "Height": 0.027620358392596245,
                        "Left": 0.12032857537269592,
                        "Top": 0.6909106373786926
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "5d85eff4-4325-46fe-bf5c-9cd97149d0e8",
                    "Score": 0.8247489929199219
                },
                {
                    "Text": "Hinges on the one-to-one relationship between a and Pn. Has shown significant improvement over BC, particularly when few expert trajectories are available or expert trajectories are subsampled.",
                    "BoundingBox": {
                        "Width": 0.7562621235847473,
                        "Height": 0.05457383021712303,
                        "Left": 0.1512637883424759,
                        "Top": 0.8884523510932922
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "0d02d0ec-ff50-4497-80fb-b62a94a4b1bf",
                    "Score": 0.8015263676643372
                }
            ]
        },
        {
            "start_time": "00:00:15,254",
            "end_time": "00:00:21,096",
            "transcript": "They are also foundational to other applications like curiosity-based exploration, constrained RL, batch RL, and convex RL.",
            "MatchedRegion": [
                {
                    "Text": "Other: Curiosity based exploration [Pathak et al. 2017]; Constrained RL [Qin et al. 2021]; Batch \"offline\" RL [Fujimoto et al. 2019]; Convex RL [Mutti et al. 2022]",
                    "BoundingBox": {
                        "Width": 0.742006242275238,
                        "Height": 0.05811062455177307,
                        "Left": 0.12042369693517685,
                        "Top": 0.5258774757385254
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "1794e711-42fb-4af8-b430-76c94b0af241",
                    "Score": 0.8033683896064758
                },
                {
                    "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
                    "BoundingBox": {
                        "Width": 0.8082616329193115,
                        "Height": 0.06645342707633972,
                        "Left": 0.08673549443483353,
                        "Top": 0.6041567921638489
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "50255d04-07c8-4a5d-8823-990b60678720",
                    "Score": 0.8130738735198975
                },
                {
                    "Text": "This work concentrates on modeling them explicitly with normalizing flows, focusing on imitation learning.",
                    "BoundingBox": {
                        "Width": 0.7311545014381409,
                        "Height": 0.027620358392596245,
                        "Left": 0.12032857537269592,
                        "Top": 0.6909106373786926
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "5d85eff4-4325-46fe-bf5c-9cd97149d0e8",
                    "Score": 0.8247489929199219
                },
                {
                    "Text": "Hinges on the one-to-one relationship between a and Pn. Has shown significant improvement over BC, particularly when few expert trajectories are available or expert trajectories are subsampled.",
                    "BoundingBox": {
                        "Width": 0.7562621235847473,
                        "Height": 0.05457383021712303,
                        "Left": 0.1512637883424759,
                        "Top": 0.8884523510932922
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "0d02d0ec-ff50-4497-80fb-b62a94a4b1bf",
                    "Score": 0.8015263676643372
                }
            ]
        },
        {
            "start_time": "00:00:21,616",
            "end_time": "00:00:28,739",
            "transcript": "Despite this ubiquity across the literature, the distributions are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
            "MatchedRegion": [
                {
                    "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
                    "BoundingBox": {
                        "Width": 0.8082616329193115,
                        "Height": 0.06645342707633972,
                        "Left": 0.08673549443483353,
                        "Top": 0.6041567921638489
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "50255d04-07c8-4a5d-8823-990b60678720",
                    "Score": 0.8130738735198975
                },
                {
                    "Text": "This work concentrates on modeling them explicitly with normalizing flows, focusing on imitation learning.",
                    "BoundingBox": {
                        "Width": 0.7311545014381409,
                        "Height": 0.027620358392596245,
                        "Left": 0.12032857537269592,
                        "Top": 0.6909106373786926
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "5d85eff4-4325-46fe-bf5c-9cd97149d0e8",
                    "Score": 0.8247489929199219
                }
            ]
        },
        {
            "start_time": "00:00:29,219",
            "end_time": "00:00:35,621",
            "transcript": "And so this would concentrate on modeling them explicitly with modern density estimators, specifically normalizing flows, focusing on imitation learning.",
            "MatchedRegion": [
                {
                    "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
                    "BoundingBox": {
                        "Width": 0.8082616329193115,
                        "Height": 0.06645342707633972,
                        "Left": 0.08673549443483353,
                        "Top": 0.6041567921638489
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "50255d04-07c8-4a5d-8823-990b60678720",
                    "Score": 0.8130738735198975
                },
                {
                    "Text": "This work concentrates on modeling them explicitly with normalizing flows, focusing on imitation learning.",
                    "BoundingBox": {
                        "Width": 0.7311545014381409,
                        "Height": 0.027620358392596245,
                        "Left": 0.12032857537269592,
                        "Top": 0.6909106373786926
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "5d85eff4-4325-46fe-bf5c-9cd97149d0e8",
                    "Score": 0.8247489929199219
                }
            ]
        },
        {
            "start_time": "00:00:36,806",
            "end_time": "00:00:43,188",
            "transcript": "The simplest approach to imitation is behavioral cloning, which reformulates supervised regression and maximum likelihood on given expert state-action pairs.",
            "MatchedRegion": [
                {
                    "Text": "In RL & IL, the agent's policy induces a state distribution d(s) and state-action distribution P(s,a) = n(as).dz(s) .",
                    "BoundingBox": {
                        "Width": 0.8002861142158508,
                        "Height": 0.06716174632310867,
                        "Left": 0.08649010211229324,
                        "Top": 0.2614990472793579
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "43e58ab5-41bc-44cc-89af-8b3ae8109207",
                    "Score": 0.8185226321220398
                },
                {
                    "Text": "All distribution matching approaches in imitation learning [Ke et al. 2020]",
                    "BoundingBox": {
                        "Width": 0.5102139115333557,
                        "Height": 0.027813950553536415,
                        "Left": 0.12038874626159668,
                        "Top": 0.478464812040329
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "92180d2e-5ed3-448d-9593-4272f2e8e7b8",
                    "Score": 0.8025819063186646
                },
                {
                    "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
                    "BoundingBox": {
                        "Width": 0.8082616329193115,
                        "Height": 0.06645342707633972,
                        "Left": 0.08673549443483353,
                        "Top": 0.6041567921638489
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "50255d04-07c8-4a5d-8823-990b60678720",
                    "Score": 0.8130738735198975
                },
                {
                    "Text": "This work concentrates on modeling them explicitly with normalizing flows, focusing on imitation learning.",
                    "BoundingBox": {
                        "Width": 0.7311545014381409,
                        "Height": 0.027620358392596245,
                        "Left": 0.12032857537269592,
                        "Top": 0.6909106373786926
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "5d85eff4-4325-46fe-bf5c-9cd97149d0e8",
                    "Score": 0.8247489929199219
                },
                {
                    "Text": "Hinges on the one-to-one relationship between a and Pn. Has shown significant improvement over BC, particularly when few expert trajectories are available or expert trajectories are subsampled.",
                    "BoundingBox": {
                        "Width": 0.7562621235847473,
                        "Height": 0.05457383021712303,
                        "Left": 0.1512637883424759,
                        "Top": 0.8884523510932922
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "0d02d0ec-ff50-4497-80fb-b62a94a4b1bf",
                    "Score": 0.8015263676643372
                }
            ]
        },
        {
            "start_time": "00:00:43,909",
            "end_time": "00:00:49,971",
            "transcript": "Modern approaches to imitation instead attempt to match the agent's state action distribution with the experts by minimizing some f divergence.",
            "MatchedRegion": [
                {
                    "Text": "In RL & IL, the agent's policy induces a state distribution d(s) and state-action distribution P(s,a) = n(as).dz(s) .",
                    "BoundingBox": {
                        "Width": 0.8002861142158508,
                        "Height": 0.06716174632310867,
                        "Left": 0.08649010211229324,
                        "Top": 0.2614990472793579
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "43e58ab5-41bc-44cc-89af-8b3ae8109207",
                    "Score": 0.8185226321220398
                },
                {
                    "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
                    "BoundingBox": {
                        "Width": 0.8082616329193115,
                        "Height": 0.06645342707633972,
                        "Left": 0.08673549443483353,
                        "Top": 0.6041567921638489
                    },
                    "BlockType": "LAYOUT_TEXT",
                    "Id": "50255d04-07c8-4a5d-8823-990b60678720",
                    "Score": 0.8130738735198975
                }
            ]
        }
    ]
}