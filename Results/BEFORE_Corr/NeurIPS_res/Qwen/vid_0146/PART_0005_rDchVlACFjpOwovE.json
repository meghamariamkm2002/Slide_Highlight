{
    "MatchedSlides": [
        "PART_0002_nY6pwgFoBjJsFRrI",
        "PART_0003_qCGccaKBz8R0BVrt"
    ],
    "MatchedRegions": [
        {
            "start_time": "00:00:01,570",
            "end_time": "00:00:11,719",
            "transcript": "We noted in order to apply loss terms on the relevance of the foreground in the background, our method requires segmentation maps to distinguish between the background and the foreground of the input image.",
            "MatchedRegion": [
                {
                    "Text": "Our method requires segmentation maps to distinguish between the background (Lbg) and the foreground (Lfg) of the image.",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.8229653835296631,
                        "Height": 0.08493687957525253,
                        "Left": 0.07626617699861526,
                        "Top": 0.2223481833934784
                    },
                    "Id": "9367633f-95f8-4bde-acf9-64615434bad0"
                }
            ]
        },
        {
            "start_time": "00:00:12,659",
            "end_time": "00:00:17,704",
            "transcript": "We use either human annotated maps or maps produced by dyno without human supervision.",
            "MatchedRegion": [
                {
                    "Text": "We use either human annotated maps or maps produced by DINO (without human supervision)",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.6961609721183777,
                        "Height": 0.030535826459527016,
                        "Left": 0.11408528685569763,
                        "Top": 0.3177497386932373
                    },
                    "Id": "252570be-53c2-414d-8f37-7f57b04aac46"
                }
            ]
        },
        {
            "start_time": "00:00:18,344",
            "end_time": "00:00:21,307",
            "transcript": "Both cases result in a similar improvement to robustness.",
            "MatchedRegion": []
        },
        {
            "start_time": "00:00:22,487",
            "end_time": "00:00:27,812",
            "transcript": "Our method only uses three examples from half the image in classes during the fine-tuning process.",
            "MatchedRegion": [
                {
                    "Text": "Our method only uses 3 examples from half the ImageNet classes (500 classes) during the finetuning process (overall only 1500 training samples).",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.8131532073020935,
                        "Height": 0.07661750912666321,
                        "Left": 0.07569033652544022,
                        "Top": 0.36154407262802124
                    },
                    "Id": "7bf5e142-971d-418d-87dc-61b8756719bb"
                }
            ]
        },
        {
            "start_time": "00:00:28,260",
            "end_time": "00:00:31,599",
            "transcript": "meaning an overall of only 1500 training samples.",
            "MatchedRegion": [
                {
                    "Text": "Our method only uses 3 examples from half the ImageNet classes (500 classes) during the finetuning process (overall only 1500 training samples).",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.8131532073020935,
                        "Height": 0.07661750912666321,
                        "Left": 0.07569033652544022,
                        "Top": 0.36154407262802124
                    },
                    "Id": "7bf5e142-971d-418d-87dc-61b8756719bb"
                }
            ]
        }
    ]
}