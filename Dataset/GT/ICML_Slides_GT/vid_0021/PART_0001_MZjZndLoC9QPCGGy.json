{
  "MatchedRegions": [
      {
          "start_time": "00:00:01,210",
          "end_time": "00:00:06,476",
          "transcript": "In reinforcement and imitation learning, the agent's policy induces a discounted state distribution and state action distribution.",
          "MatchedRegion": [
            {
              "Id": "43e58ab5-41bc-44cc-89af-8b3ae8109207",
              "Text": "In RL & IL, the agent's policy induces a state distribution d(s) and state-action distribution P(s,a) = n(as).dz(s) .",
              "BoundingBox": {
                  "Width": 0.8002861142158508,
                  "Height": 0.06716174632310867,
                  "Left": 0.08649010211229324,
                  "Top": 0.2614990472793579
              },
              "BlockType": "LAYOUT_TEXT"
            }
          ]
      },
      {
        "start_time": "00:00:07,011",
        "end_time": "00:00:09,672",
        "transcript": "They are of central importance, appearing all across the literature.",
        "MatchedRegion": [
            {
              "Id": "a91c0711-79e1-4748-afc4-5c2a6ecbc1b9",
              "Text": "They are of central importance, appearing all across the literature:",
              "BoundingBox": {
                  "Width": 0.5164313316345215,
                  "Height": 0.03070523776113987,
                  "Left": 0.08655297011137009,
                  "Top": 0.34929364919662476
              },
              "BlockType": "LAYOUT_TEXT"
            }
        ]
    },
    {
      "start_time": "00:00:09,692",
      "end_time": "00:00:14,694",
      "transcript": "They form the basis of policy gradient methods in RL and are core to the distribution matching formulation of imitation.",
      "MatchedRegion": [
        {
          "Id": "7b8ba6ad-7795-4ee7-8a88-6f692066633c",
          "Text": "The Policy Gradient Theorem: A fundamental theorem from which all policy-based methods are derived. [Sutton et al., 2000]",
          "BoundingBox": {
              "Width": 0.783841073513031,
              "Height": 0.057240210473537445,
              "Left": 0.12033142149448395,
              "Top": 0.4007152318954468
          },
          "BlockType": "LAYOUT_TEXT"
        }
      ]
    },
    {
      "start_time": "00:00:15,254",
      "end_time": "00:00:21,096",
      "transcript": "They are also foundational to other applications like curiosity-based exploration, constrained RL, batch RL, and convex RL.",
      "MatchedRegion": [
        {
          "Id": "1794e711-42fb-4af8-b430-76c94b0af241",
          "Text": "Other: Curiosity based exploration [Pathak et al. 2017]; Constrained RL [Qin et al. 2021]; Batch \"offline\" RL [Fujimoto et al. 2019]; Convex RL [Mutti et al. 2022]",
          "BoundingBox": {
              "Width": 0.742006242275238,
              "Height": 0.05811062455177307,
              "Left": 0.12042369693517685,
              "Top": 0.5258774757385254
          },
          "BlockType": "LAYOUT_TEXT"
        }
      ]
    },
    {
      "start_time": "00:00:21,616",
      "end_time": "00:00:28,739",
      "transcript": "Despite this ubiquity across the literature, the distributions are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
      "MatchedRegion": [
        {
          "Id": "50255d04-07c8-4a5d-8823-990b60678720",
          "Text": "Despite their importance, d (s) and P(s,a) are mostly discussed indirectly and theoretically, rather than being modeled explicitly.",
          "BoundingBox": {
              "Width": 0.8082616329193115,
              "Height": 0.06645342707633972,
              "Left": 0.08673549443483353,
              "Top": 0.6041567921638489
          },
          "BlockType": "LAYOUT_TEXT"
        }
      ]
    },
    {
      "start_time": "00:00:29,219",
      "end_time": "00:00:35,621",
      "transcript": "And so this would concentrate on modeling them explicitly with modern density estimators, specifically normalizing flows, focusing on imitation.",
      "MatchedRegion": [
        {
          "Id": "5d85eff4-4325-46fe-bf5c-9cd97149d0e8",
          "Text": "This work concentrates on modeling them explicitly with normalizing flows, focusing on imitation learning.",
          "BoundingBox": {
              "Width": 0.7311545014381409,
              "Height": 0.027620358392596245,
              "Left": 0.12032857537269592,
              "Top": 0.6909106373786926
          },
          "BlockType": "LAYOUT_TEXT"
        }
      ]
    },
    {
      "start_time": "00:00:36,806",
      "end_time": "00:00:43,188",
      "transcript": "The simplest approach to imitation is behavioral cloning, which reforms supervised regression and maximum likelihood on given expert state action pairs.",
      "MatchedRegion": [
        {
          "Id": "4ecb6ee9-69c8-412a-be19-9a4692d3feba",
          "Text": "Simple approach: Behavioral cloning (BC)",
          "BoundingBox": {
              "Width": 0.30025961995124817,
              "Height": 0.02745872549712658,
              "Left": 0.12025824934244156,
              "Top": 0.7916000485420227
          },
          "BlockType": "LAYOUT_TEXT"
        }
      ]
    },
    {
      "start_time": "00:00:43,909",
      "end_time": "00:00:49,971",
      "transcript": "Modern approaches to imitation instead attempt to match the agent's state action distribution with the experts by minimizing some f diverged",
      "MatchedRegion": [
        {
          "Id": "33d6d2ba-4b0d-4c54-84c8-cc3a19fbfbb8",
          "Text": "Distribution matching: minDf(P || Pexp)",
          "BoundingBox": {
              "Width": 0.29278114438056946,
              "Height": 0.030913962051272392,
              "Left": 0.12029442191123962,
              "Top": 0.8406777381896973
          },
          "BlockType": "LAYOUT_TEXT"
        }
      ]
    }
  ],
  "MatchedSlides": []
}