{
    "MatchedSlides": [
        "PART_0001_LZGyyKU5zYJCxbZS",
        "PART_0002_nY6pwgFoBjJsFRrI"
    ],
    "MatchedRegions": [
        {
            "start_time": "00:00:01,775",
            "end_time": "00:00:04,876",
            "transcript": "First, let's describe the problem the paper aims to address.",
            "MatchedRegion": [
                {
                    "Text": "Problem Definition",
                    "BlockType": "LAYOUT_TITLE",
                    "BoundingBox": {
                        "Width": 0.33384355902671814,
                        "Height": 0.06120728701353073,
                        "Left": 0.07934241741895676,
                        "Top": 0.11338723450899124
                    },
                    "Id": "03c3a26a-4a23-405c-ab37-21ad08d6e3b9"
                }
            ]
        },
        {
            "start_time": "00:00:05,757",
            "end_time": "00:00:10,238",
            "transcript": "It is a known fact that vision models tend to rely on spurious cues to make their predictions.",
            "MatchedRegion": [
                {
                    "Text": "Vision models tend to rely on spurious cues to make predictions",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.5881450176239014,
                        "Height": 0.03345668315887451,
                        "Left": 0.07638096064329147,
                        "Top": 0.216069296002388
                    },
                    "Id": "0bef55a9-bef0-4514-b5b7-494538acef6b"
                }
            ]
        },
        {
            "start_time": "00:00:11,419",
            "end_time": "00:00:17,501",
            "transcript": "In this work, we examine the spurious cues used by vision transformers and find that they suffer from two main issues.",
            "MatchedRegion": [
                {
                    "Text": "We find that Vision Transformers often suffer from two main salient issues:",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.6789045929908752,
                        "Height": 0.02804054133594036,
                        "Left": 0.07598841935396194,
                        "Top": 0.2696458697319031
                    },
                    "Id": "6a970fec-3f28-4a71-bec4-83cc7b7280f4"
                }
            ]
        },
        {
            "start_time": "00:00:18,362",
            "end_time": "00:00:24,864",
            "transcript": "The first is overreliance on the image background, and the second is a sparse consideration of the foreground pixels.",
            "MatchedRegion": [
                {
                    "Text": "Over reliance on the background",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.267190545797348,
                        "Height": 0.027338843792676926,
                        "Left": 0.11370924860239029,
                        "Top": 0.3146696984767914
                    },
                    "Id": "65561184-1647-4722-89eb-cb7a93c48d5f"
                },
                {
                    "Text": "Sparse consideration of the foreground pixels",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.35242870450019836,
                        "Height": 0.02780582197010517,
                        "Left": 0.1138836070895195,
                        "Top": 0.35135188698768616
                    },
                    "Id": "64be519e-84fe-4139-8d91-47c978383f06"
                }
            ]
        },
        {
            "start_time": "00:00:25,865",
            "end_time": "00:00:28,306",
            "transcript": "The figure here represents examples of both cases.",
            "MatchedRegion": []
        },
        {
            "start_time": "00:00:29,130",
            "end_time": "00:00:40,176",
            "transcript": "For example, the image here is classified by the model as a chestnut with a confidence of 100%, but the saliency map shows that the model used only the background to make its prediction.",
            "MatchedRegion": [
                {
                    "Text": "Classification: Chestnut Confidence: 100%",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.11274172365665436,
                        "Height": 0.04296521469950676,
                        "Left": 0.26203203201293945,
                        "Top": 0.5821345448493958
                    },
                    "Id": "d77bf38f-5d40-468a-ba57-66a5011343aa"
                }
            ]
        },
        {
            "start_time": "00:00:41,577",
            "end_time": "00:00:45,979",
            "transcript": "This can lead to lack of robustness to distribution shifts and poor generalization.",
            "MatchedRegion": [
                {
                    "Text": "This leads to lack of robustness to distribution shifts, i.e., poor generalization",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.6976851224899292,
                        "Height": 0.03496260195970535,
                        "Left": 0.07587239146232605,
                        "Top": 0.831281840801239
                    },
                    "Id": "d8cdf1be-405b-4d1b-85da-0d1c3a5f547d"
                }
            ]
        },
        {
            "start_time": "00:00:46,759",
            "end_time": "00:00:49,521",
            "transcript": "We ask ourselves, can we correct the model's logic?",
            "MatchedRegion": [
                {
                    "Text": "Can we correct the model's logic?",
                    "BlockType": "LAYOUT_TEXT",
                    "BoundingBox": {
                        "Width": 0.273681640625,
                        "Height": 0.02824333868920803,
                        "Left": 0.11327361315488815,
                        "Top": 0.8762390613555908
                    },
                    "Id": "19c46297-9ec5-4948-bf03-db6241190901"
                }
            ]
        }
    ]
}