WEBVTT

00:01.570 --> 00:11.719
We noted in order to apply loss terms on the relevance of the foreground in the background, our method requires segmentation maps to distinguish between the background and the foreground of the input image.

00:12.659 --> 00:17.704
We use either human annotated maps or maps produced by dyno without human supervision.

00:18.344 --> 00:21.307
Both cases result in a similar improvement to robustness.

00:22.487 --> 00:27.812
Our method only uses three examples from half the image in classes during the fine-tuning process.

00:28.260 --> 00:31.599
meaning an overall of only 1500 training samples.

