{"segments": [{"start": 1.033, "end": 9.075, "text": " Unlike other methods, our method employs the losses during a short fine-tuning process on models that were already pre-trained on ImageNet.", "words": [{"word": "Unlike", "start": 1.033, "end": 1.373, "score": 0.895}, {"word": "other", "start": 1.473, "end": 1.673, "score": 0.712}, {"word": "methods,", "start": 1.693, "end": 2.113, "score": 0.765}, {"word": "our", "start": 2.493, "end": 2.593, "score": 0.702}, {"word": "method", "start": 2.633, "end": 2.954, "score": 0.902}, {"word": "employs", "start": 3.014, "end": 3.394, "score": 0.804}, {"word": "the", "start": 3.434, "end": 3.514, "score": 0.83}, {"word": "losses", "start": 3.554, "end": 3.994, "score": 0.854}, {"word": "during", "start": 4.114, "end": 4.374, "score": 0.915}, {"word": "a", "start": 4.434, "end": 4.474, "score": 0.496}, {"word": "short", "start": 4.534, "end": 4.794, "score": 0.895}, {"word": "fine-tuning", "start": 4.854, "end": 5.394, "score": 0.767}, {"word": "process", "start": 5.414, "end": 5.894, "score": 0.881}, {"word": "on", "start": 6.435, "end": 6.515, "score": 0.861}, {"word": "models", "start": 6.575, "end": 6.855, "score": 0.974}, {"word": "that", "start": 6.895, "end": 7.055, "score": 0.782}, {"word": "were", "start": 7.075, "end": 7.235, "score": 0.765}, {"word": "already", "start": 7.355, "end": 7.715, "score": 0.905}, {"word": "pre-trained", "start": 7.815, "end": 8.375, "score": 0.884}, {"word": "on", "start": 8.435, "end": 8.515, "score": 0.921}, {"word": "ImageNet.", "start": 8.615, "end": 9.075, "score": 0.85}]}, {"start": 10.016, "end": 17.418, "text": "We add a classification loss at classification to ensure that the accuracy of the models does not decrease due to the fine-tuning process.", "words": [{"word": "We", "start": 10.016, "end": 10.136, "score": 0.986}, {"word": "add", "start": 10.256, "end": 10.396, "score": 0.889}, {"word": "a", "start": 10.416, "end": 10.436, "score": 0.956}, {"word": "classification", "start": 10.476, "end": 11.236, "score": 0.875}, {"word": "loss", "start": 11.276, "end": 11.556, "score": 0.875}, {"word": "at", "start": 11.756, "end": 11.856, "score": 0.745}, {"word": "classification", "start": 11.876, "end": 12.696, "score": 0.901}, {"word": "to", "start": 12.997, "end": 13.097, "score": 0.996}, {"word": "ensure", "start": 13.137, "end": 13.417, "score": 0.862}, {"word": "that", "start": 13.457, "end": 13.577, "score": 0.873}, {"word": "the", "start": 13.617, "end": 13.717, "score": 0.81}, {"word": "accuracy", "start": 13.857, "end": 14.317, "score": 0.851}, {"word": "of", "start": 14.357, "end": 14.437, "score": 0.79}, {"word": "the", "start": 14.477, "end": 14.557, "score": 0.81}, {"word": "models", "start": 14.617, "end": 14.937, "score": 0.943}, {"word": "does", "start": 14.977, "end": 15.157, "score": 0.744}, {"word": "not", "start": 15.217, "end": 15.377, "score": 0.986}, {"word": "decrease", "start": 15.417, "end": 15.857, "score": 0.918}, {"word": "due", "start": 16.017, "end": 16.177, "score": 0.869}, {"word": "to", "start": 16.217, "end": 16.297, "score": 0.716}, {"word": "the", "start": 16.337, "end": 16.397, "score": 0.966}, {"word": "fine-tuning", "start": 16.458, "end": 16.958, "score": 0.765}, {"word": "process.", "start": 16.998, "end": 17.418, "score": 0.845}]}, {"start": 18.418, "end": 24.8, "text": "The formulation of our classification loss is as follows, and it encourages the model to have high confidence on its predictions.", "words": [{"word": "The", "start": 18.418, "end": 18.518, "score": 0.845}, {"word": "formulation", "start": 18.538, "end": 19.118, "score": 0.926}, {"word": "of", "start": 19.158, "end": 19.218, "score": 0.75}, {"word": "our", "start": 19.278, "end": 19.358, "score": 0.836}, {"word": "classification", "start": 19.398, "end": 20.099, "score": 0.923}, {"word": "loss", "start": 20.139, "end": 20.379, "score": 0.859}, {"word": "is", "start": 20.459, "end": 20.539, "score": 0.647}, {"word": "as", "start": 20.619, "end": 20.679, "score": 0.862}, {"word": "follows,", "start": 20.739, "end": 21.139, "score": 0.897}, {"word": "and", "start": 21.339, "end": 21.459, "score": 0.811}, {"word": "it", "start": 21.499, "end": 21.559, "score": 0.748}, {"word": "encourages", "start": 21.639, "end": 22.199, "score": 0.806}, {"word": "the", "start": 22.219, "end": 22.319, "score": 0.279}, {"word": "model", "start": 22.359, "end": 22.699, "score": 0.943}, {"word": "to", "start": 22.779, "end": 22.899, "score": 0.803}, {"word": "have", "start": 22.939, "end": 23.139, "score": 0.834}, {"word": "high", "start": 23.179, "end": 23.46, "score": 0.791}, {"word": "confidence", "start": 23.52, "end": 24.02, "score": 0.956}, {"word": "on", "start": 24.08, "end": 24.14, "score": 0.964}, {"word": "its", "start": 24.18, "end": 24.28, "score": 0.709}, {"word": "predictions.", "start": 24.32, "end": 24.8, "score": 0.878}]}, {"start": 25.97, "end": 32.154, "text": " Thus, our final loss term is formulated as follows as a combination of the relevance and the classification loss.", "words": [{"word": "Thus,", "start": 25.97, "end": 26.23, "score": 0.895}, {"word": "our", "start": 26.43, "end": 26.53, "score": 0.913}, {"word": "final", "start": 26.59, "end": 26.911, "score": 0.891}, {"word": "loss", "start": 26.931, "end": 27.151, "score": 0.756}, {"word": "term", "start": 27.171, "end": 27.451, "score": 0.818}, {"word": "is", "start": 27.551, "end": 27.631, "score": 0.668}, {"word": "formulated", "start": 27.671, "end": 28.212, "score": 0.874}, {"word": "as", "start": 28.292, "end": 28.352, "score": 0.962}, {"word": "follows", "start": 28.432, "end": 28.772, "score": 0.852}, {"word": "as", "start": 28.972, "end": 29.052, "score": 0.818}, {"word": "a", "start": 29.092, "end": 29.132, "score": 0.49}, {"word": "combination", "start": 29.172, "end": 29.813, "score": 0.9}, {"word": "of", "start": 29.993, "end": 30.073, "score": 0.836}, {"word": "the", "start": 30.113, "end": 30.173, "score": 0.993}, {"word": "relevance", "start": 30.233, "end": 30.693, "score": 0.717}, {"word": "and", "start": 31.034, "end": 31.134, "score": 0.863}, {"word": "the", "start": 31.154, "end": 31.214, "score": 0.983}, {"word": "classification", "start": 31.254, "end": 31.934, "score": 0.923}, {"word": "loss.", "start": 31.974, "end": 32.154, "score": 0.637}]}], "word_segments": [{"word": "Unlike", "start": 1.033, "end": 1.373, "score": 0.895}, {"word": "other", "start": 1.473, "end": 1.673, "score": 0.712}, {"word": "methods,", "start": 1.693, "end": 2.113, "score": 0.765}, {"word": "our", "start": 2.493, "end": 2.593, "score": 0.702}, {"word": "method", "start": 2.633, "end": 2.954, "score": 0.902}, {"word": "employs", "start": 3.014, "end": 3.394, "score": 0.804}, {"word": "the", "start": 3.434, "end": 3.514, "score": 0.83}, {"word": "losses", "start": 3.554, "end": 3.994, "score": 0.854}, {"word": "during", "start": 4.114, "end": 4.374, "score": 0.915}, {"word": "a", "start": 4.434, "end": 4.474, "score": 0.496}, {"word": "short", "start": 4.534, "end": 4.794, "score": 0.895}, {"word": "fine-tuning", "start": 4.854, "end": 5.394, "score": 0.767}, {"word": "process", "start": 5.414, "end": 5.894, "score": 0.881}, {"word": "on", "start": 6.435, "end": 6.515, "score": 0.861}, {"word": "models", "start": 6.575, "end": 6.855, "score": 0.974}, {"word": "that", "start": 6.895, "end": 7.055, "score": 0.782}, {"word": "were", "start": 7.075, "end": 7.235, "score": 0.765}, {"word": "already", "start": 7.355, "end": 7.715, "score": 0.905}, {"word": "pre-trained", "start": 7.815, "end": 8.375, "score": 0.884}, {"word": "on", "start": 8.435, "end": 8.515, "score": 0.921}, {"word": "ImageNet.", "start": 8.615, "end": 9.075, "score": 0.85}, {"word": "We", "start": 10.016, "end": 10.136, "score": 0.986}, {"word": "add", "start": 10.256, "end": 10.396, "score": 0.889}, {"word": "a", "start": 10.416, "end": 10.436, "score": 0.956}, {"word": "classification", "start": 10.476, "end": 11.236, "score": 0.875}, {"word": "loss", "start": 11.276, "end": 11.556, "score": 0.875}, {"word": "at", "start": 11.756, "end": 11.856, "score": 0.745}, {"word": "classification", "start": 11.876, "end": 12.696, "score": 0.901}, {"word": "to", "start": 12.997, "end": 13.097, "score": 0.996}, {"word": "ensure", "start": 13.137, "end": 13.417, "score": 0.862}, {"word": "that", "start": 13.457, "end": 13.577, "score": 0.873}, {"word": "the", "start": 13.617, "end": 13.717, "score": 0.81}, {"word": "accuracy", "start": 13.857, "end": 14.317, "score": 0.851}, {"word": "of", "start": 14.357, "end": 14.437, "score": 0.79}, {"word": "the", "start": 14.477, "end": 14.557, "score": 0.81}, {"word": "models", "start": 14.617, "end": 14.937, "score": 0.943}, {"word": "does", "start": 14.977, "end": 15.157, "score": 0.744}, {"word": "not", "start": 15.217, "end": 15.377, "score": 0.986}, {"word": "decrease", "start": 15.417, "end": 15.857, "score": 0.918}, {"word": "due", "start": 16.017, "end": 16.177, "score": 0.869}, {"word": "to", "start": 16.217, "end": 16.297, "score": 0.716}, {"word": "the", "start": 16.337, "end": 16.397, "score": 0.966}, {"word": "fine-tuning", "start": 16.458, "end": 16.958, "score": 0.765}, {"word": "process.", "start": 16.998, "end": 17.418, "score": 0.845}, {"word": "The", "start": 18.418, "end": 18.518, "score": 0.845}, {"word": "formulation", "start": 18.538, "end": 19.118, "score": 0.926}, {"word": "of", "start": 19.158, "end": 19.218, "score": 0.75}, {"word": "our", "start": 19.278, "end": 19.358, "score": 0.836}, {"word": "classification", "start": 19.398, "end": 20.099, "score": 0.923}, {"word": "loss", "start": 20.139, "end": 20.379, "score": 0.859}, {"word": "is", "start": 20.459, "end": 20.539, "score": 0.647}, {"word": "as", "start": 20.619, "end": 20.679, "score": 0.862}, {"word": "follows,", "start": 20.739, "end": 21.139, "score": 0.897}, {"word": "and", "start": 21.339, "end": 21.459, "score": 0.811}, {"word": "it", "start": 21.499, "end": 21.559, "score": 0.748}, {"word": "encourages", "start": 21.639, "end": 22.199, "score": 0.806}, {"word": "the", "start": 22.219, "end": 22.319, "score": 0.279}, {"word": "model", "start": 22.359, "end": 22.699, "score": 0.943}, {"word": "to", "start": 22.779, "end": 22.899, "score": 0.803}, {"word": "have", "start": 22.939, "end": 23.139, "score": 0.834}, {"word": "high", "start": 23.179, "end": 23.46, "score": 0.791}, {"word": "confidence", "start": 23.52, "end": 24.02, "score": 0.956}, {"word": "on", "start": 24.08, "end": 24.14, "score": 0.964}, {"word": "its", "start": 24.18, "end": 24.28, "score": 0.709}, {"word": "predictions.", "start": 24.32, "end": 24.8, "score": 0.878}, {"word": "Thus,", "start": 25.97, "end": 26.23, "score": 0.895}, {"word": "our", "start": 26.43, "end": 26.53, "score": 0.913}, {"word": "final", "start": 26.59, "end": 26.911, "score": 0.891}, {"word": "loss", "start": 26.931, "end": 27.151, "score": 0.756}, {"word": "term", "start": 27.171, "end": 27.451, "score": 0.818}, {"word": "is", "start": 27.551, "end": 27.631, "score": 0.668}, {"word": "formulated", "start": 27.671, "end": 28.212, "score": 0.874}, {"word": "as", "start": 28.292, "end": 28.352, "score": 0.962}, {"word": "follows", "start": 28.432, "end": 28.772, "score": 0.852}, {"word": "as", "start": 28.972, "end": 29.052, "score": 0.818}, {"word": "a", "start": 29.092, "end": 29.132, "score": 0.49}, {"word": "combination", "start": 29.172, "end": 29.813, "score": 0.9}, {"word": "of", "start": 29.993, "end": 30.073, "score": 0.836}, {"word": "the", "start": 30.113, "end": 30.173, "score": 0.993}, {"word": "relevance", "start": 30.233, "end": 30.693, "score": 0.717}, {"word": "and", "start": 31.034, "end": 31.134, "score": 0.863}, {"word": "the", "start": 31.154, "end": 31.214, "score": 0.983}, {"word": "classification", "start": 31.254, "end": 31.934, "score": 0.923}, {"word": "loss.", "start": 31.974, "end": 32.154, "score": 0.637}], "language": "en"}