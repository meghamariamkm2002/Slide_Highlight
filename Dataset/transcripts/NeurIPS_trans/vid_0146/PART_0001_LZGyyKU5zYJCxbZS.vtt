WEBVTT

00:01.775 --> 00:04.876
First, let's describe the problem the paper tries to deal with.

00:05.757 --> 00:10.238
It is a known fact that vision models tend to rely on spurious cues to make their predictions.

00:11.419 --> 00:17.501
In this work, we examine the spurious cues used by vision transformers and find that they suffer from two main issues.

00:18.362 --> 00:24.864
The first is overreliance on the image background, and the second is a sparse consideration of the foreground pixels.

00:25.865 --> 00:28.306
The figure here represents examples of both cases.

00:29.130 --> 00:40.176
For example, the image here is classified by the model as a chestnut with a confidence of 100%, but the saliency map shows that the model used only the background to make its prediction.

00:41.577 --> 00:45.979
This can lead to lack of robustness to distribution shifts and poor generalization.

00:46.759 --> 00:49.521
We ask ourselves, can we correct the model's logic?

