1
00:00:00,009 --> 00:00:00,469
algorithms.

2
00:00:03,090 --> 00:00:05,711
This will give an overview of the block-wise last methods.

3
00:00:06,591 --> 00:00:18,996
First, block-wise distillation divides a pre-trained reference model, known as teacher, into sequential blocks that are later distributed into a library of pre-trained replacement blocks together with their signatures.

4
00:00:20,536 --> 00:00:28,659
Two, search uses this signature to guide an algorithm to find a well-performing model built by stacking a number of blocks from the block library.

5
00:00:29,983 --> 00:00:38,982
Third, fine tuning is a process when block of student model are initialized with weights obtained in distillation, that the model are trained with knowledge distributed.

