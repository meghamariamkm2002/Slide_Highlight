algorithms.
This will give an overview of the block-wise last methods.
First, block-wise distillation divides a pre-trained reference model, known as teacher, into sequential blocks that are later distributed into a library of pre-trained replacement blocks together with their signatures.
Two, search uses this signature to guide an algorithm to find a well-performing model built by stacking a number of blocks from the block library.
Third, fine tuning is a process when block of student model are initialized with weights obtained in distillation, that the model are trained with knowledge distributed.
