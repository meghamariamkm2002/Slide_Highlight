{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method : String Matching:Fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text = 0.8, figure = 0.6, model = sbert, overlay = 60%\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Parent directories\n",
    "image_parent_dir = \"/tmp/megha/Complete/slides/ICML_slides\"\n",
    "transcript_parent_dir = \"/tmp/megha/Complete/transcripts/ICML_trans\"\n",
    "aws_ocr_parent_dir = \"/tmp/megha/Complete/Layout/ICML_layout/json\"\n",
    "output_parent_dir = \"/tmp/megha/Complete/WITHOUT/ICML_res/Fuzzy/T-1\"\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# time from srt\n",
    "def parse_srt(file_path):\n",
    "    \"\"\"Extracts timestamps and text from an SRT file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    entries = []\n",
    "    srt_pattern = re.compile(r\"(\\d+)\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.*?)\\n\\n\", re.DOTALL)\n",
    "    \n",
    "    for match in srt_pattern.finditer(content):\n",
    "        index, start_time, end_time, text = match.groups()\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        entries.append({\"start_time\": start_time, \"end_time\": end_time, \"text\": text})\n",
    "    \n",
    "    return entries\n",
    "\n",
    "# to get ocr_regions - Any ways it was avoided\n",
    "def load_aws_ocr(file_path):\n",
    "    \"\"\"Loads AWS OCR results and extracts recognized text with bounding boxes and IDs.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    ocr_text_map = {\n",
    "        block[\"Id\"]: block.get(\"Text\", \"\") \n",
    "        for block in data.get(\"Blocks\", []) \n",
    "        if block.get(\"BlockType\") == \"LINE\"\n",
    "    }\n",
    "\n",
    "    regions = []\n",
    "    for block in data.get(\"Blocks\", []):\n",
    "        if block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\"):\n",
    "            child_blocks = [\n",
    "                child_id \n",
    "                for rel in block.get(\"Relationships\", []) \n",
    "                if rel[\"Type\"] == \"CHILD\" \n",
    "                for child_id in rel.get(\"Ids\", [])\n",
    "            ]\n",
    "            \n",
    "            # Check if any child block is also a LAYOUT_ type\n",
    "            if any(\n",
    "                child_block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\") \n",
    "                for child_block in data.get(\"Blocks\", []) \n",
    "                if child_block[\"Id\"] in child_blocks\n",
    "            ):\n",
    "                print(f\"Parent block {block['Id']} has layout children, hence skipped.\")\n",
    "                continue\n",
    "\n",
    "            text = [ocr_text_map[child_id] for child_id in child_blocks if child_id in ocr_text_map]\n",
    "            \n",
    "            regions.append({\n",
    "                \"Text\": \" \".join(text),\n",
    "                \"BoundingBox\": block[\"Geometry\"][\"BoundingBox\"],\n",
    "                \"BlockType\": block[\"BlockType\"],\n",
    "                \"Id\": block['Id']\n",
    "            })\n",
    "\n",
    "    return regions\n",
    "\n",
    "# finding best matches\n",
    "def get_best_matches(transcript_text, ocr_regions):\n",
    "    \"\"\"Finds the best matching OCR regions for a transcript line using fuzzy matching.\"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    for region in ocr_regions:\n",
    "        ocr_text = region[\"Text\"].strip()\n",
    "        if not ocr_text:\n",
    "            continue\n",
    "        \n",
    "        # Fuzzy matching score (0-100 scale)\n",
    "        score = fuzz.ratio(transcript_text, ocr_text) / 100.0\n",
    "        \n",
    "        threshold = 0.8 if region[\"BlockType\"] != \"LAYOUT_FIGURE\" else 0.6\n",
    "        if score > threshold:\n",
    "            region['Score'] = score\n",
    "            matches.append(region)\n",
    "    \n",
    "    return sorted(matches, key=lambda x: x[\"Score\"], reverse=True)\n",
    "\n",
    "# region matching\n",
    "def match_transcript_to_regions(srt_entries, ocr_regions, model):\n",
    "    \"\"\"Matches transcript lines to OCR regions.\"\"\"\n",
    "    matched_results = []\n",
    "    for entry in srt_entries:\n",
    "        matches = get_best_matches(entry[\"text\"], ocr_regions)\n",
    "        print(f'The no. of regions: {len(matches)} for entry: {entry}')\n",
    "        matched_results.append({\"start_time\": entry[\"start_time\"], \"end_time\": entry[\"end_time\"], \"transcript\": entry[\"text\"], \"MatchedRegion\": matches})\n",
    "    return matched_results\n",
    "\n",
    "\n",
    "# for getting entire ocr data\n",
    "def extract_ocr_text(json_data):\n",
    "    \"\"\"Extract full OCR text from AWS OCR JSON.\"\"\"\n",
    "    block_map = {block[\"Id\"]: block for block in json_data[\"Blocks\"]}\n",
    "    ocr_text = []\n",
    "    \n",
    "    for block in json_data[\"Blocks\"]:\n",
    "        if block[\"BlockType\"] == \"PAGE\" and \"Relationships\" in block:\n",
    "            for relation in block[\"Relationships\"]:\n",
    "                if relation[\"Type\"] == \"CHILD\":\n",
    "                    for child_id in relation[\"Ids\"]:\n",
    "                        child_block = block_map.get(child_id)\n",
    "                        if child_block and \"Text\" in child_block:\n",
    "                            ocr_text.append(child_block[\"Text\"])\n",
    "    \n",
    "    return \" \".join(ocr_text)\n",
    "\n",
    "\n",
    "# save overlay and regions\n",
    "def save_results(matched_results, output_json):\n",
    "    \"\"\"Saves the matched results as JSON.\"\"\"\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"MatchedRegions\": matched_results}, file, indent=4)\n",
    "\n",
    "\n",
    "# Gather all transcript embeddings for full-slide similarity\n",
    "full_transcripts = {}\n",
    "for sub_dir in sorted(os.listdir(transcript_parent_dir)):  # Sorting for consistency\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    if os.path.isdir(sub_transcript_dir):\n",
    "        for file in sorted(os.listdir(sub_transcript_dir)):  # Sorting filenames\n",
    "            if file.endswith(\".srt\"):\n",
    "                file_path = os.path.join(sub_transcript_dir, file)\n",
    "                text = \" \".join([entry[\"text\"] for entry in parse_srt(file_path)])\n",
    "                full_transcripts[file] = {\"text\": text, \"embedding\": model.encode(text, convert_to_tensor=True)}\n",
    "\n",
    "\n",
    "# Iterate through slides for processing\n",
    "for sub_dir in sorted(os.listdir(image_parent_dir)):  # Sorting for consistency\n",
    "    sub_image_dir = os.path.join(image_parent_dir, sub_dir)\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    sub_aws_ocr_dir = os.path.join(aws_ocr_parent_dir, sub_dir)\n",
    "    output_dir = os.path.join(output_parent_dir, sub_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not (os.path.isdir(sub_image_dir) and os.path.isdir(sub_transcript_dir) and os.path.isdir(sub_aws_ocr_dir)):\n",
    "        continue\n",
    "    \n",
    "    for file in sorted(os.listdir(sub_image_dir)):  # Sorting filenames\n",
    "        if file.endswith(\".png\"):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            print(f'This is the filename: {base_name}')\n",
    "            image_path = os.path.join(sub_image_dir, file)\n",
    "            transcript_path = os.path.join(sub_transcript_dir, f\"{base_name}.srt\")\n",
    "            ocr_path = os.path.join(sub_aws_ocr_dir, f\"{base_name}.json\")\n",
    "            output_json = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "            output_image = os.path.join(output_dir, f\"{base_name}_bbox.png\")\n",
    "            \n",
    "            if os.path.exists(transcript_path) and os.path.exists(ocr_path):\n",
    "                srt_entries = parse_srt(transcript_path)\n",
    "                ocr_regions = load_aws_ocr(ocr_path)\n",
    "                #print(ocr_regions)\n",
    "                #print(full_transcripts)\n",
    "                # Step 1: Match transcript lines to OCR regions\n",
    "                matched_results = match_transcript_to_regions(srt_entries, ocr_regions, model)\n",
    "\n",
    "                # Save\n",
    "                save_results(matched_results, output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: Semantic Matching:S-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text = 0.8, figure = 0.6, model = sbert, overlay = 60%\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Parent directories\n",
    "image_parent_dir = \"/tmp/megha/Complete/slides/ICML_slides\"\n",
    "transcript_parent_dir = \"/tmp/megha/Complete/transcripts/ICML_trans\"\n",
    "aws_ocr_parent_dir = \"/tmp/megha/Complete/Layout/ICML_layout/json\"\n",
    "output_parent_dir = \"/tmp/megha/Complete/WITHOUT/ICML_res/S-BERT/T-1\"\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# time from srt\n",
    "def parse_srt(file_path):\n",
    "    \"\"\"Extracts timestamps and text from an SRT file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    entries = []\n",
    "    srt_pattern = re.compile(r\"(\\d+)\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.*?)\\n\\n\", re.DOTALL)\n",
    "    \n",
    "    for match in srt_pattern.finditer(content):\n",
    "        index, start_time, end_time, text = match.groups()\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        entries.append({\"start_time\": start_time, \"end_time\": end_time, \"text\": text})\n",
    "    \n",
    "    return entries\n",
    "\n",
    "# to get ocr_regions - Any ways it was avoided\n",
    "def load_aws_ocr(file_path):\n",
    "    \"\"\"Loads AWS OCR results and extracts recognized text with bounding boxes and IDs.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    ocr_text_map = {\n",
    "        block[\"Id\"]: block.get(\"Text\", \"\") \n",
    "        for block in data.get(\"Blocks\", []) \n",
    "        if block.get(\"BlockType\") == \"LINE\"\n",
    "    }\n",
    "\n",
    "    regions = []\n",
    "    for block in data.get(\"Blocks\", []):\n",
    "        if block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\"):\n",
    "            child_blocks = [\n",
    "                child_id \n",
    "                for rel in block.get(\"Relationships\", []) \n",
    "                if rel[\"Type\"] == \"CHILD\" \n",
    "                for child_id in rel.get(\"Ids\", [])\n",
    "            ]\n",
    "            \n",
    "            # Check if any child block is also a LAYOUT_ type\n",
    "            if any(\n",
    "                child_block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\") \n",
    "                for child_block in data.get(\"Blocks\", []) \n",
    "                if child_block[\"Id\"] in child_blocks\n",
    "            ):\n",
    "                print(f\"Parent block {block['Id']} has layout children, hence skipped.\")\n",
    "                continue\n",
    "\n",
    "            text = [ocr_text_map[child_id] for child_id in child_blocks if child_id in ocr_text_map]\n",
    "            \n",
    "            regions.append({\n",
    "                \"Text\": \" \".join(text),\n",
    "                \"BoundingBox\": block[\"Geometry\"][\"BoundingBox\"],\n",
    "                \"BlockType\": block[\"BlockType\"],\n",
    "                \"Id\": block['Id']\n",
    "            })\n",
    "\n",
    "    return regions\n",
    "\n",
    "# finding best matches\n",
    "def get_best_matches(transcript_text, ocr_regions, model):\n",
    "    \"\"\"Finds the best matching OCR regions for a transcript line.\"\"\"\n",
    "    transcript_embedding = model.encode(transcript_text, convert_to_tensor=True)\n",
    "    matches = []\n",
    "    \n",
    "    for region in ocr_regions:\n",
    "        ocr_text = region[\"Text\"].strip()\n",
    "        if not ocr_text:\n",
    "            continue\n",
    "        ocr_embedding = model.encode(ocr_text, convert_to_tensor=True)\n",
    "        score = util.pytorch_cos_sim(transcript_embedding, ocr_embedding).item()\n",
    "        threshold = 0.8 if region[\"BlockType\"] != \"LAYOUT_FIGURE\" else 0.6\n",
    "        if score > threshold:\n",
    "            region['Score'] = score\n",
    "            matches.append(region)\n",
    "    \n",
    "    return sorted(matches, key=lambda x: x[\"Score\"], reverse=True)\n",
    "\n",
    "# region matching\n",
    "def match_transcript_to_regions(srt_entries, ocr_regions, model):\n",
    "    \"\"\"Matches transcript lines to OCR regions.\"\"\"\n",
    "    matched_results = []\n",
    "    for entry in srt_entries:\n",
    "        matches = get_best_matches(entry[\"text\"], ocr_regions, model)\n",
    "        print(f'The no. of regions: {len(matches)} for entry: {entry}')\n",
    "        matched_results.append({\"start_time\": entry[\"start_time\"], \"end_time\": entry[\"end_time\"], \"transcript\": entry[\"text\"], \"MatchedRegion\": matches})\n",
    "    return matched_results\n",
    "\n",
    "\n",
    "# for getting entire ocr data\n",
    "def extract_ocr_text(json_data):\n",
    "    \"\"\"Extract full OCR text from AWS OCR JSON.\"\"\"\n",
    "    block_map = {block[\"Id\"]: block for block in json_data[\"Blocks\"]}\n",
    "    ocr_text = []\n",
    "    \n",
    "    for block in json_data[\"Blocks\"]:\n",
    "        if block[\"BlockType\"] == \"PAGE\" and \"Relationships\" in block:\n",
    "            for relation in block[\"Relationships\"]:\n",
    "                if relation[\"Type\"] == \"CHILD\":\n",
    "                    for child_id in relation[\"Ids\"]:\n",
    "                        child_block = block_map.get(child_id)\n",
    "                        if child_block and \"Text\" in child_block:\n",
    "                            ocr_text.append(child_block[\"Text\"])\n",
    "    \n",
    "    return \" \".join(ocr_text)\n",
    "\n",
    "\n",
    "# save overlay and regions\n",
    "def save_results(matched_results, matched_slides, output_json):\n",
    "    \"\"\"Saves the matched results as JSON.\"\"\"\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"MatchedRegions\": matched_results}, file, indent=4)\n",
    "\n",
    "\n",
    "# Gather all transcript embeddings for full-slide similarity\n",
    "full_transcripts = {}\n",
    "for sub_dir in sorted(os.listdir(transcript_parent_dir)):  # Sorting for consistency\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    if os.path.isdir(sub_transcript_dir):\n",
    "        for file in sorted(os.listdir(sub_transcript_dir)):  # Sorting filenames\n",
    "            if file.endswith(\".srt\"):\n",
    "                file_path = os.path.join(sub_transcript_dir, file)\n",
    "                text = \" \".join([entry[\"text\"] for entry in parse_srt(file_path)])\n",
    "                full_transcripts[file] = {\"text\": text, \"embedding\": model.encode(text, convert_to_tensor=True)}\n",
    "\n",
    "\n",
    "# Iterate through slides for processing\n",
    "for sub_dir in sorted(os.listdir(image_parent_dir)):  # Sorting for consistency\n",
    "    sub_image_dir = os.path.join(image_parent_dir, sub_dir)\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    sub_aws_ocr_dir = os.path.join(aws_ocr_parent_dir, sub_dir)\n",
    "    output_dir = os.path.join(output_parent_dir, sub_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not (os.path.isdir(sub_image_dir) and os.path.isdir(sub_transcript_dir) and os.path.isdir(sub_aws_ocr_dir)):\n",
    "        continue\n",
    "    \n",
    "    for file in sorted(os.listdir(sub_image_dir)):  # Sorting filenames\n",
    "        if file.endswith(\".png\"):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            print(f'This is the filename: {base_name}')\n",
    "            image_path = os.path.join(sub_image_dir, file)\n",
    "            transcript_path = os.path.join(sub_transcript_dir, f\"{base_name}.srt\")\n",
    "            ocr_path = os.path.join(sub_aws_ocr_dir, f\"{base_name}.json\")\n",
    "            output_json = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "            output_image = os.path.join(output_dir, f\"{base_name}_bbox.png\")\n",
    "            \n",
    "            if os.path.exists(transcript_path) and os.path.exists(ocr_path):\n",
    "                srt_entries = parse_srt(transcript_path)\n",
    "                ocr_regions = load_aws_ocr(ocr_path)\n",
    "                #print(ocr_regions)\n",
    "                #print(full_transcripts)\n",
    "                # Step 1: Match transcript lines to OCR regions\n",
    "                matched_results = match_transcript_to_regions(srt_entries, ocr_regions, model)\n",
    "                save_results(matched_results, output_json)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: Semantic Matching:Sci-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 0.8\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Parent directories\n",
    "image_parent_dir = \"/tmp/megha/Complete/slides/ICML_slides\"\n",
    "transcript_parent_dir = \"/tmp/megha/Complete/Corr_trans/ICML_corr_trans\"\n",
    "aws_ocr_parent_dir = \"/tmp/megha/Complete/Layout/ICML_layout/json\"\n",
    "output_parent_dir = \"/tmp/megha/Complete/WITHOUT/ICML_res/Sci-BERT/T-1\"\n",
    "\n",
    "\n",
    "# Load the SPECTER model\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "MAX_TOKENS = 512  # SciBERT's actual token limit\n",
    "\n",
    "# time from srt\n",
    "def parse_srt(file_path):\n",
    "    \"\"\"Extracts timestamps and text from an SRT file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    entries = []\n",
    "    srt_pattern = re.compile(r\"(\\d+)\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.*?)\\n\\n\", re.DOTALL)\n",
    "    \n",
    "    for match in srt_pattern.finditer(content):\n",
    "        index, start_time, end_time, text = match.groups()\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        entries.append({\"start_time\": start_time, \"end_time\": end_time, \"text\": text})\n",
    "    \n",
    "    return entries\n",
    "\n",
    "# to get ocr_regions\n",
    "def load_aws_ocr(file_path):\n",
    "    \"\"\"Loads AWS OCR results and extracts recognized text with bounding boxes and IDs.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    ocr_text_map = {\n",
    "        block[\"Id\"]: block.get(\"Text\", \"\") \n",
    "        for block in data.get(\"Blocks\", []) \n",
    "        if block.get(\"BlockType\") == \"LINE\"\n",
    "    }\n",
    "\n",
    "    regions = []\n",
    "    for block in data.get(\"Blocks\", []):\n",
    "        if block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\"):\n",
    "            child_blocks = [\n",
    "                child_id \n",
    "                for rel in block.get(\"Relationships\", []) \n",
    "                if rel[\"Type\"] == \"CHILD\" \n",
    "                for child_id in rel.get(\"Ids\", [])\n",
    "            ]\n",
    "            \n",
    "            # Check if any child block is also a LAYOUT_ type\n",
    "            if any(\n",
    "                child_block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\") \n",
    "                for child_block in data.get(\"Blocks\", []) \n",
    "                if child_block[\"Id\"] in child_blocks\n",
    "            ):\n",
    "                print(f\"Parent block {block['Id']} has layout children, hence skipped.\")\n",
    "                continue\n",
    "\n",
    "            text = [ocr_text_map[child_id] for child_id in child_blocks if child_id in ocr_text_map]\n",
    "            \n",
    "            regions.append({\n",
    "                \"Text\": \" \".join(text),\n",
    "                \"BoundingBox\": block[\"Geometry\"][\"BoundingBox\"],\n",
    "                \"BlockType\": block[\"BlockType\"],\n",
    "                \"Id\": block['Id']\n",
    "            })\n",
    "\n",
    "    return regions\n",
    "\n",
    "# finding best matches\n",
    "def get_embedding(text):\n",
    "    \"\"\"Tokenizes text and gets its embedding using SPECTER, printing if truncation occurs.\"\"\"\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    if tokens['input_ids'].shape[1] == 512:\n",
    "        print(f\"Truncation occurred for text: {text[:50]}...\")  # Print truncated text preview\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**tokens).last_hidden_state[:, 0, :]\n",
    "    return embedding\n",
    "\n",
    "def get_best_matches(transcript_text, ocr_regions):\n",
    "    \"\"\"Finds the best matching OCR regions for a transcript line using SPECTER.\"\"\"\n",
    "    transcript_embedding = get_embedding(transcript_text)\n",
    "    matches = []\n",
    "    \n",
    "    for region in ocr_regions:\n",
    "        ocr_text = region.get(\"Text\").strip()  # Ensure we always get a string\n",
    "        if not ocr_text:\n",
    "            print(\"Skipping empty OCR text\")  # Confirmation message\n",
    "            continue\n",
    "        else:\n",
    "            print('Text not skipped')\n",
    "        \n",
    "        ocr_embedding = get_embedding(ocr_text)\n",
    "        score = util.pytorch_cos_sim(transcript_embedding, ocr_embedding).item()\n",
    "        threshold = 0.8 if region[\"BlockType\"] != \"LAYOUT_FIGURE\" else 0.6\n",
    "\n",
    "        print(f'This is the score: {score}')\n",
    "        \n",
    "        if score > threshold:\n",
    "            print(f'This is the score: {score}')\n",
    "            region['Score'] = score\n",
    "            matches.append(region)\n",
    "\n",
    "    print(f'These are the matches: {matches}, length of matches: {len(matches)}')\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def match_transcript_to_regions(srt_entries, ocr_regions):\n",
    "    \"\"\"Matches transcript lines to OCR regions using SPECTER.\"\"\"\n",
    "    matched_results = []\n",
    "    \n",
    "    for entry in srt_entries:\n",
    "        matches = get_best_matches(entry[\"text\"], ocr_regions)\n",
    "        print(f'Matched inside mat: {matches}')\n",
    "        matched_results.append({\n",
    "            \"start_time\": entry[\"start_time\"],\n",
    "            \"end_time\": entry[\"end_time\"],\n",
    "            \"transcript\": entry[\"text\"],\n",
    "            \"MatchedRegion\": matches\n",
    "        })\n",
    "\n",
    "    print(f'Matched results before: {matched_results}')\n",
    "    return matched_results\n",
    "\n",
    "# for getting entire ocr data\n",
    "def extract_ocr_text(json_data):\n",
    "    \"\"\"Extract full OCR text from AWS OCR JSON.\"\"\"\n",
    "    block_map = {block[\"Id\"]: block for block in json_data[\"Blocks\"]}\n",
    "    ocr_text = []\n",
    "    \n",
    "    for block in json_data[\"Blocks\"]:\n",
    "        if block[\"BlockType\"] == \"PAGE\" and \"Relationships\" in block:\n",
    "            for relation in block[\"Relationships\"]:\n",
    "                if relation[\"Type\"] == \"CHILD\":\n",
    "                    for child_id in relation[\"Ids\"]:\n",
    "                        child_block = block_map.get(child_id)\n",
    "                        if child_block and \"Text\" in child_block:\n",
    "                            ocr_text.append(child_block[\"Text\"])\n",
    "    \n",
    "    return \" \".join(ocr_text)\n",
    "\n",
    "def save_results(matched_results, output_json):\n",
    "    \"\"\"Saves the matched results as JSON.\"\"\"\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"MatchedRegions\": matched_results}, file, indent=4)\n",
    "\n",
    "\n",
    "def check_truncation(text, tokenizer):\n",
    "    \"\"\"Check if text exceeds SciBERT's token limit and print a warning if truncation is needed.\"\"\"\n",
    "    try:\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\", truncation=False)  # No truncation to check actual length\n",
    "        token_len = tokenized_text.input_ids.shape[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'The error while calculating token len: {e}')\n",
    "    if token_len > tokenizer.model_max_length:\n",
    "        print(f\"Warning: Text exceeds SciBERT token limit ({token_len} > {tokenizer.model_max_length}) and may be truncated.\")\n",
    "    \n",
    "    return token_len  # Return actual token length for further checks\n",
    "\n",
    "\n",
    "# Gather all transcript embeddings for full-slide similarity\n",
    "full_transcripts = {}\n",
    "for sub_dir in sorted(os.listdir(transcript_parent_dir)):  # Sorting for consistency\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    if os.path.isdir(sub_transcript_dir):\n",
    "        for file in sorted(os.listdir(sub_transcript_dir)):  # Sorting filenames\n",
    "            if file.endswith(\".srt\"):\n",
    "                file_path = os.path.join(sub_transcript_dir, file)\n",
    "                text = \" \".join([entry[\"text\"] for entry in parse_srt(file_path)])\n",
    "                tokenized_text = check_truncation(text, tokenizer)\n",
    "                full_transcripts[file] = {\n",
    "                    \"text\": text,\n",
    "                    \"embedding\":  get_embedding(text)\n",
    "                }\n",
    "\n",
    "# Iterate through slides for processing\n",
    "for sub_dir in sorted(os.listdir(image_parent_dir)):  # Sorting for consistency\n",
    "    sub_image_dir = os.path.join(image_parent_dir, sub_dir)\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    sub_aws_ocr_dir = os.path.join(aws_ocr_parent_dir, sub_dir)\n",
    "    output_dir = os.path.join(output_parent_dir, sub_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not (os.path.isdir(sub_image_dir) and os.path.isdir(sub_transcript_dir) and os.path.isdir(sub_aws_ocr_dir)):\n",
    "        continue\n",
    "    \n",
    "    for file in sorted(os.listdir(sub_image_dir)):  # Sorting filenames\n",
    "        if file.endswith(\".png\"):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            image_path = os.path.join(sub_image_dir, file)\n",
    "            transcript_path = os.path.join(sub_transcript_dir, f\"{base_name}.srt\")\n",
    "            ocr_path = os.path.join(sub_aws_ocr_dir, f\"{base_name}.json\")\n",
    "            output_json = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "            output_image = os.path.join(output_dir, f\"{base_name}_bbox.png\")\n",
    "            \n",
    "            if os.path.exists(transcript_path) and os.path.exists(ocr_path):\n",
    "                srt_entries = parse_srt(transcript_path)\n",
    "                ocr_regions = load_aws_ocr(ocr_path)\n",
    "                print(ocr_regions)\n",
    "                #print(full_transcripts)\n",
    "                # Step 1: Match transcript lines to OCR regions\n",
    "                matched_results = match_transcript_to_regions(srt_entries, ocr_regions)\n",
    "                print(f'This is the matched regions: {matched_results}')\n",
    "\n",
    "                save_results(matched_results, output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: Semantic Matching:SPECTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text = 0.8, figure = 0.6, model = specter, overlay = 60%\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Parent directories\n",
    "image_parent_dir = \"/tmp/megha/Complete/slides/ICML_slides\"\n",
    "transcript_parent_dir = \"/tmp/megha/Complete/transcripts/ICML_trans\"\n",
    "aws_ocr_parent_dir = \"/tmp/megha/Complete/Layout/ICML_layout/json\"\n",
    "output_parent_dir = \"/tmp/megha/Complete/WITHOUT/ICML_res/SPECTER/T-1\"\n",
    "\n",
    "\n",
    "# Load the SPECTER model\n",
    "model = AutoModel.from_pretrained('allenai/specter')\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "MAX_TOKENS = 512  # SciBERT's actual token limit\n",
    "\n",
    "# time from srt\n",
    "def parse_srt(file_path):\n",
    "    \"\"\"Extracts timestamps and text from an SRT file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    entries = []\n",
    "    srt_pattern = re.compile(r\"(\\d+)\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.*?)\\n\\n\", re.DOTALL)\n",
    "    \n",
    "    for match in srt_pattern.finditer(content):\n",
    "        index, start_time, end_time, text = match.groups()\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        entries.append({\"start_time\": start_time, \"end_time\": end_time, \"text\": text})\n",
    "    \n",
    "    return entries\n",
    "\n",
    "# to get ocr_regions\n",
    "def load_aws_ocr(file_path):\n",
    "    \"\"\"Loads AWS OCR results and extracts recognized text with bounding boxes and IDs.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    ocr_text_map = {\n",
    "        block[\"Id\"]: block.get(\"Text\", \"\") \n",
    "        for block in data.get(\"Blocks\", []) \n",
    "        if block.get(\"BlockType\") == \"LINE\"\n",
    "    }\n",
    "\n",
    "    regions = []\n",
    "    for block in data.get(\"Blocks\", []):\n",
    "        if block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\"):\n",
    "            child_blocks = [\n",
    "                child_id \n",
    "                for rel in block.get(\"Relationships\", []) \n",
    "                if rel[\"Type\"] == \"CHILD\" \n",
    "                for child_id in rel.get(\"Ids\", [])\n",
    "            ]\n",
    "            \n",
    "            # Check if any child block is also a LAYOUT_ type\n",
    "            if any(\n",
    "                child_block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\") \n",
    "                for child_block in data.get(\"Blocks\", []) \n",
    "                if child_block[\"Id\"] in child_blocks\n",
    "            ):\n",
    "                print(f\"Parent block {block['Id']} has layout children, hence skipped.\")\n",
    "                continue\n",
    "\n",
    "            text = [ocr_text_map[child_id] for child_id in child_blocks if child_id in ocr_text_map]\n",
    "            \n",
    "            regions.append({\n",
    "                \"Text\": \" \".join(text),\n",
    "                \"BoundingBox\": block[\"Geometry\"][\"BoundingBox\"],\n",
    "                \"BlockType\": block[\"BlockType\"],\n",
    "                \"Id\": block['Id']\n",
    "            })\n",
    "\n",
    "    return regions\n",
    "\n",
    "# finding best matches\n",
    "def get_embedding(text):\n",
    "    \"\"\"Tokenizes text and gets its embedding using SPECTER, printing if truncation occurs.\"\"\"\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    if tokens['input_ids'].shape[1] == 512:\n",
    "        print(f\"Truncation occurred for text: {text[:50]}...\")  # Print truncated text preview\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**tokens).last_hidden_state[:, 0, :]\n",
    "    return embedding\n",
    "\n",
    "def get_best_matches(transcript_text, ocr_regions):\n",
    "    \"\"\"Finds the best matching OCR regions for a transcript line using SPECTER.\"\"\"\n",
    "    transcript_embedding = get_embedding(transcript_text)\n",
    "    matches = []\n",
    "    \n",
    "    for region in ocr_regions:\n",
    "        ocr_text = region.get(\"Text\").strip()  # Ensure we always get a string\n",
    "        if not ocr_text:\n",
    "            print(\"Skipping empty OCR text\")  # Confirmation message\n",
    "            continue\n",
    "        else:\n",
    "            print('Text not skipped')\n",
    "        \n",
    "        ocr_embedding = get_embedding(ocr_text)\n",
    "        score = util.pytorch_cos_sim(transcript_embedding, ocr_embedding).item()\n",
    "        threshold = 0.8 if region[\"BlockType\"] != \"LAYOUT_FIGURE\" else 0.6\n",
    "\n",
    "        print(f'This is the score: {score}')\n",
    "        \n",
    "        if score > threshold:\n",
    "            print(f'This is the score: {score}')\n",
    "            region['Score'] = score\n",
    "            matches.append(region)\n",
    "\n",
    "    print(f'These are the matches: {matches}, length of matches: {len(matches)}')\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def match_transcript_to_regions(srt_entries, ocr_regions):\n",
    "    \"\"\"Matches transcript lines to OCR regions using SPECTER.\"\"\"\n",
    "    matched_results = []\n",
    "    \n",
    "    for entry in srt_entries:\n",
    "        matches = get_best_matches(entry[\"text\"], ocr_regions)\n",
    "        print(f'Matched inside mat: {matches}')\n",
    "        matched_results.append({\n",
    "            \"start_time\": entry[\"start_time\"],\n",
    "            \"end_time\": entry[\"end_time\"],\n",
    "            \"transcript\": entry[\"text\"],\n",
    "            \"MatchedRegion\": matches\n",
    "        })\n",
    "\n",
    "    print(f'Matched results before: {matched_results}')\n",
    "    return matched_results\n",
    "\n",
    "# for getting entire ocr data\n",
    "def extract_ocr_text(json_data):\n",
    "    \"\"\"Extract full OCR text from AWS OCR JSON.\"\"\"\n",
    "    block_map = {block[\"Id\"]: block for block in json_data[\"Blocks\"]}\n",
    "    ocr_text = []\n",
    "    \n",
    "    for block in json_data[\"Blocks\"]:\n",
    "        if block[\"BlockType\"] == \"PAGE\" and \"Relationships\" in block:\n",
    "            for relation in block[\"Relationships\"]:\n",
    "                if relation[\"Type\"] == \"CHILD\":\n",
    "                    for child_id in relation[\"Ids\"]:\n",
    "                        child_block = block_map.get(child_id)\n",
    "                        if child_block and \"Text\" in child_block:\n",
    "                            ocr_text.append(child_block[\"Text\"])\n",
    "    \n",
    "    return \" \".join(ocr_text)\n",
    "\n",
    "# save overlay and regions\n",
    "def save_results(matched_results, output_json):\n",
    "    \"\"\"Saves the matched results as JSON.\"\"\"\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"MatchedRegion\": matched_results}, file, indent=4)\n",
    "\n",
    "\n",
    "def check_truncation(text, tokenizer):\n",
    "    \"\"\"Check if text exceeds SciBERT's token limit and print a warning if truncation is needed.\"\"\"\n",
    "    try:\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\", truncation=False)  # No truncation to check actual length\n",
    "        token_len = tokenized_text.input_ids.shape[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'The error while calculating token len: {e}')\n",
    "    if token_len > tokenizer.model_max_length:\n",
    "        print(f\"Warning: Text exceeds SciBERT token limit ({token_len} > {tokenizer.model_max_length}) and may be truncated.\")\n",
    "    \n",
    "    return token_len  # Return actual token length for further checks\n",
    "\n",
    "\n",
    "# Gather all transcript embeddings for full-slide similarity\n",
    "full_transcripts = {}\n",
    "for sub_dir in sorted(os.listdir(transcript_parent_dir)):  # Sorting for consistency\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    if os.path.isdir(sub_transcript_dir):\n",
    "        for file in sorted(os.listdir(sub_transcript_dir)):  # Sorting filenames\n",
    "            if file.endswith(\".srt\"):\n",
    "                file_path = os.path.join(sub_transcript_dir, file)\n",
    "                text = \" \".join([entry[\"text\"] for entry in parse_srt(file_path)])\n",
    "                tokenized_text = check_truncation(text, tokenizer)\n",
    "                full_transcripts[file] = {\n",
    "                    \"text\": text,\n",
    "                    \"embedding\":  get_embedding(text)\n",
    "                }\n",
    "\n",
    "# Iterate through slides for processing\n",
    "for sub_dir in sorted(os.listdir(image_parent_dir)):  # Sorting for consistency\n",
    "    sub_image_dir = os.path.join(image_parent_dir, sub_dir)\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    sub_aws_ocr_dir = os.path.join(aws_ocr_parent_dir, sub_dir)\n",
    "    output_dir = os.path.join(output_parent_dir, sub_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not (os.path.isdir(sub_image_dir) and os.path.isdir(sub_transcript_dir) and os.path.isdir(sub_aws_ocr_dir)):\n",
    "        continue\n",
    "    \n",
    "    for file in sorted(os.listdir(sub_image_dir)):  # Sorting filenames\n",
    "        if file.endswith(\".png\"):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            image_path = os.path.join(sub_image_dir, file)\n",
    "            transcript_path = os.path.join(sub_transcript_dir, f\"{base_name}.srt\")\n",
    "            ocr_path = os.path.join(sub_aws_ocr_dir, f\"{base_name}.json\")\n",
    "            output_json = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "            output_image = os.path.join(output_dir, f\"{base_name}_bbox.png\")\n",
    "            \n",
    "            if os.path.exists(transcript_path) and os.path.exists(ocr_path):\n",
    "                srt_entries = parse_srt(transcript_path)\n",
    "                ocr_regions = load_aws_ocr(ocr_path)\n",
    "                print(ocr_regions)\n",
    "                #print(full_transcripts)\n",
    "                # Step 1: Match transcript lines to OCR regions\n",
    "                matched_results = match_transcript_to_regions(srt_entries, ocr_regions)\n",
    "                print(f'This is the matched regions: {matched_results}')\n",
    "                save_results(matched_results, output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: Semantic Matching:T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import ast\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Parent directories\n",
    "image_parent_dir = \"/tmp/megha/Complete/slides/ICML_slides\"\n",
    "transcript_parent_dir = \"/tmp/megha/Complete/transcripts/ICML_trans\"\n",
    "aws_ocr_parent_dir = \"/tmp/megha/Complete/Layout/ICML_layout/json\"\n",
    "output_parent_dir = \"/tmp/megha/Complete/WITHOUT/ICML_res/T5\"\n",
    "\n",
    "# time from srt\n",
    "def parse_srt(file_path):\n",
    "    \"\"\"Extracts timestamps and text from an SRT file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    entries = []\n",
    "    srt_pattern = re.compile(r\"(\\d+)\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.*?)\\n\\n\", re.DOTALL)\n",
    "    \n",
    "    for match in srt_pattern.finditer(content):\n",
    "        index, start_time, end_time, text = match.groups()\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        entries.append({\"start_time\": start_time, \"end_time\": end_time, \"text\": text})\n",
    "    \n",
    "    return entries\n",
    "\n",
    "# to get ocr_regions\n",
    "def load_aws_ocr(file_path):\n",
    "    \"\"\"Loads AWS OCR results and extracts recognized text with bounding boxes and IDs.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    ocr_text_map = {\n",
    "        block[\"Id\"]: block.get(\"Text\", \"\") \n",
    "        for block in data.get(\"Blocks\", []) \n",
    "        if block.get(\"BlockType\") == \"LINE\"\n",
    "    }\n",
    "\n",
    "    regions = []\n",
    "    for block in data.get(\"Blocks\", []):\n",
    "        if block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\"):\n",
    "            child_blocks = [\n",
    "                child_id \n",
    "                for rel in block.get(\"Relationships\", []) \n",
    "                if rel[\"Type\"] == \"CHILD\" \n",
    "                for child_id in rel.get(\"Ids\", [])\n",
    "            ]\n",
    "            \n",
    "            # Check if any child block is also a LAYOUT_ type\n",
    "            if any(\n",
    "                child_block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\") \n",
    "                for child_block in data.get(\"Blocks\", []) \n",
    "                if child_block[\"Id\"] in child_blocks\n",
    "            ):\n",
    "                print(f\"Parent block {block['Id']} has layout children, hence skipped.\")\n",
    "                continue\n",
    "\n",
    "            text = [ocr_text_map[child_id] for child_id in child_blocks if child_id in ocr_text_map]\n",
    "            \n",
    "            regions.append({\n",
    "                \"Text\": \" \".join(text),\n",
    "                \"BoundingBox\": block[\"Geometry\"][\"BoundingBox\"],\n",
    "                \"BlockType\": block[\"BlockType\"],\n",
    "                \"Id\": block['Id']\n",
    "            })\n",
    "\n",
    "    return regions\n",
    "\n",
    "def extract_output_list(response):\n",
    "    \"\"\"Extracts only the list after 'Output List:' from the response.\"\"\"\n",
    "    match = re.search(r\"Output List:\\s*(\\[[^\\]]*\\])\", response)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Extract and return only the list\n",
    "    return \"[]\"  # Return an empty list if not found\n",
    "\n",
    "\n",
    "# Additional function to check for figure relevance - just string matching \n",
    "def check_figure_relevance(transcript_text, figure_text):\n",
    "    \"\"\"Checks if a LAYOUT_FIGURE contains keywords related to the transcript.\"\"\"\n",
    "    transcript_words = set(transcript_text.lower().split())\n",
    "    figure_words = set(figure_text.lower().split())\n",
    "    \n",
    "    return any(word in figure_words for word in transcript_words)\n",
    "\n",
    "# Ask flan-t5 for each ocr_text whether it is relevant or not for the transcript_text - for Region\n",
    "def check_relevance(transcript, ocr_texts):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Check available device\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "    relevant_texts_list = []\n",
    "    for ocr in ocr_texts:\n",
    "        input_text = f\"Given the transcript: {transcript}, is this OCR text relevant? {ocr_texts[ocr]['Text']}\"\n",
    "        # Calculate the number of tokens before processing\n",
    "        num_tokens = len(tokenizer.encode(input_text, truncation=False))  \n",
    "        #print(f\"Input text token count: {num_tokens}\")\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Move inputs to the same device\n",
    "        output = model.generate(**inputs)\n",
    "        result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        if \"yes\" in result.lower() or \"relevant\" in result.lower():\n",
    "            relevant_texts_list.append(ocr_texts[ocr])\n",
    "    print(f'This is the relevant_texts_dict: {relevant_texts_list}')\n",
    "    \n",
    "    return relevant_texts_list\n",
    "\n",
    "# Getting best matches\n",
    "def get_best_matches(transcript_text, ocr_regions):\n",
    "    \"\"\"Finds the best matching OCR regions for a transcript line using T5.\"\"\"\n",
    "    ocr_texts = [f\"Region {i+1}: {region['Text']} ({region['BlockType']})\" for i, region in enumerate(ocr_regions)]\n",
    "    ocr_texts_extract = {\n",
    "        f\"Region: {i+1}\": {\n",
    "            \"Text\": region[\"Text\"],\n",
    "            \"BlockType\": region[\"BlockType\"],\n",
    "            \"BoundingBox\": region[\"BoundingBox\"],\n",
    "            \"Id\": region['Id']\n",
    "        } \n",
    "        for i, region in enumerate(ocr_regions)\n",
    "    }\n",
    "    \n",
    "    #prompt = generate_prompt(transcript_text, ocr_texts_extracts)\n",
    "    # pass for flan-t5 checking\n",
    "    matched_regions = check_relevance(transcript_text, ocr_texts_extract)\n",
    "\n",
    "    \n",
    "    #matched_regions = [ocr_texts_extract[region] for region in output_list if region in ocr_texts_extract]\n",
    "    #print(f'The matched regions: {matched_regions}')\n",
    "    \n",
    "    return matched_regions\n",
    "\n",
    "\n",
    "# Slide matching\n",
    "def match_transcript_to_regions(srt_entries, ocr_regions):\n",
    "    \"\"\"Matches transcript lines to OCR regions using Qwen.\"\"\"\n",
    "    matched_results = []\n",
    "    for entry in srt_entries:\n",
    "        try:\n",
    "            matches = get_best_matches(entry[\"text\"], ocr_regions)\n",
    "            print(f'these are matches: {matches}')\n",
    "            matched_results.append({\n",
    "                \"start_time\": entry[\"start_time\"],\n",
    "                \"end_time\": entry[\"end_time\"],\n",
    "                \"transcript\": entry[\"text\"],\n",
    "                \"MatchedRegion\": [\n",
    "                    match for match in matches # Exclude empty text matches\n",
    "                ]\n",
    "            })\n",
    "            for match in matches:\n",
    "                print(f'This is match text: {match['Text']}')\n",
    "                print(f'This is bbox: {match[\"BoundingBox\"]}')\n",
    "            print(f'Formatted matched regions: {matched_results}')\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "    return matched_results\n",
    "\n",
    "\n",
    "# for getting entire ocr data\n",
    "def extract_ocr_text(json_data):\n",
    "    \"\"\"Extract full OCR text from AWS OCR JSON.\"\"\"\n",
    "    block_map = {block[\"Id\"]: block for block in json_data[\"Blocks\"]}\n",
    "    ocr_text = []\n",
    "    \n",
    "    for block in json_data[\"Blocks\"]:\n",
    "        if block[\"BlockType\"] == \"PAGE\" and \"Relationships\" in block:\n",
    "            for relation in block[\"Relationships\"]:\n",
    "                if relation[\"Type\"] == \"CHILD\":\n",
    "                    for child_id in relation[\"Ids\"]:\n",
    "                        child_block = block_map.get(child_id)\n",
    "                        if child_block and \"Text\" in child_block:\n",
    "                            ocr_text.append(child_block[\"Text\"])\n",
    "    \n",
    "    return \" \".join(ocr_text)\n",
    "\n",
    "\n",
    "# content overlap - to avoid animated slides (not used)\n",
    "def check_content_overlap(text1, text2, threshold=0.90):\n",
    "    \"\"\"Checks if at least `threshold` proportion of text1 is present in text2.\"\"\"\n",
    "    words1, words2 = text1.split(), text2.split()\n",
    "    if len(words1) == 0:\n",
    "        return False\n",
    "    match_count = sum(1 for word in words1 if word in words2)\n",
    "    return (match_count / len(words1)) >= threshold\n",
    "\n",
    "\n",
    "# Prompts for similar slides \n",
    "def generate_prompt_slides(transcript_text, current_ocr_text, formatted_ocr_texts):\n",
    "    \"\"\"Generates a prompt for Qwen to find the best matching OCR texts.\"\"\"\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Given the transcript: \\\"{transcript_text}\\\" and the OCR text: \\\"{current_ocr_text}\\\", \"\n",
    "        \"find the most relevant slides from the following list that discuss similar topics. \"\n",
    "        \"Select slides where key concepts overlap.\\n\\n\"\n",
    "        f\"Slides OCR Texts:\\n{formatted_ocr_texts}\\n\\n\"\n",
    "        \"Output format:\\n['Slide <Slide Number>', .... ]\\n\\n\"\n",
    "        \"Output List:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def save_results(matched_results, output_json):\n",
    "    \"\"\"Saves the matched results as JSON in the required format.\"\"\"\n",
    "    \n",
    "    formatted_results = []\n",
    "    \n",
    "    for result in matched_results:\n",
    "        formatted_entry = {\n",
    "            \"start_time\": result[\"start_time\"],\n",
    "            \"end_time\": result[\"end_time\"],\n",
    "            \"transcript\": result[\"transcript\"],\n",
    "            \"MatchedRegions\": []\n",
    "        }\n",
    "\n",
    "        for region in result.get(\"MatchedRegion\", []):  # Ensure it exists\n",
    "            #print(f'Inside loop: {region}')\n",
    "            #if region['Region'].get(\"Text\"):  # Only include meaningful text regions\n",
    "                #print('text exist')\n",
    "            formatted_entry[\"MatchedRegions\"].append(region)\n",
    "\n",
    "        formatted_results.append(formatted_entry)\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"MatchedRegions\": formatted_results}, file, indent=4)\n",
    "\n",
    "\n",
    "# Gather all transcript embeddings for full-slide similarity\n",
    "full_transcripts = {}\n",
    "for sub_dir in sorted(os.listdir(transcript_parent_dir)):  # Sorting for consistency\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    if os.path.isdir(sub_transcript_dir):\n",
    "        for file in sorted(os.listdir(sub_transcript_dir)):  # Sorting filenames\n",
    "            if file.endswith(\".srt\"):\n",
    "                file_path = os.path.join(sub_transcript_dir, file)\n",
    "                text = \" \".join([entry[\"text\"] for entry in parse_srt(file_path)])\n",
    "                full_transcripts[file] = {\"text\": text }\n",
    "\n",
    "\n",
    "# Iterate through slides for processing\n",
    "for sub_dir in sorted(os.listdir(image_parent_dir)):  # Sorting for consistency\n",
    "    sub_image_dir = os.path.join(image_parent_dir, sub_dir)\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    sub_aws_ocr_dir = os.path.join(aws_ocr_parent_dir, sub_dir)\n",
    "    output_dir = os.path.join(output_parent_dir, sub_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not (os.path.isdir(sub_image_dir) and os.path.isdir(sub_transcript_dir) and os.path.isdir(sub_aws_ocr_dir)):\n",
    "        continue\n",
    "    \n",
    "    for file in sorted(os.listdir(sub_image_dir)):  # Sorting filenames\n",
    "        if file.endswith(\".png\"):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            image_path = os.path.join(sub_image_dir, file)\n",
    "            transcript_path = os.path.join(sub_transcript_dir, f\"{base_name}.srt\")\n",
    "            ocr_path = os.path.join(sub_aws_ocr_dir, f\"{base_name}.json\")\n",
    "            output_json = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "            output_image = os.path.join(output_dir, f\"{base_name}_bbox.png\")\n",
    "            \n",
    "            if os.path.exists(transcript_path) and os.path.exists(ocr_path):\n",
    "                srt_entries = parse_srt(transcript_path)\n",
    "                ocr_regions = load_aws_ocr(ocr_path)\n",
    "                #print(ocr_regions)\n",
    "                #print(full_transcripts)\n",
    "                # Step 1: Match transcript lines to OCR regions\n",
    "                matched_results = match_transcript_to_regions(srt_entries, ocr_regions)\n",
    "                print(f'This is the matching regions: {matched_results}')\n",
    "\n",
    "                # Step 2: Find similar slides based on full transcript\n",
    "                #matched_slides = find_similar_slides(full_transcripts, f\"{base_name}.srt\",sub_aws_ocr_dir )\n",
    "                #print(f'This is the macthed slides: {matched_slides}')\n",
    "\n",
    "                save_results(matched_results, output_json)\n",
    "                #draw_bounding_boxes(image_path, matched_results, output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: Semantic Matching:T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM code with everything\n",
    "# REGIONS Working\n",
    "# Now will check Slides\n",
    "\n",
    "# part 8 - vid_68 had some issue (check into if time)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "# Load Qwen model and tokenizer\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "image_parent_dir = \"/ssd_scratch/cvit/megha/Dataset/slides/NeurIPS_slides\"\n",
    "transcript_parent_dir = \"/ssd_scratch/cvit/megha/Results/Corr_trans/NeurIPS_corr_trans\" # use the corrected transcripts\n",
    "aws_ocr_parent_dir = \"/ssd_scratch/cvit/megha/Dataset/Layout/NeurIPS_layout/json\"\n",
    "output_parent_dir = \"/ssd_scratch/cvit/megha/new/NeurIPS_res\"\n",
    "\n",
    "''''# Parent directories\n",
    "image_parent_dir = \"/tmp/megha/Complete/slides/ICML_slides\"\n",
    "transcript_parent_dir = \"/tmp/megha/Complete/transcripts/ICML_trans\"\n",
    "aws_ocr_parent_dir = \"/tmp/megha/Complete/Layout/ICML_layout/json\"\n",
    "output_parent_dir = \"/tmp/megha/Complete/WITHOUT/ICML_res/Qwen\"'''\n",
    "\n",
    "# time from srt\n",
    "def parse_srt(file_path):\n",
    "    \"\"\"Extracts timestamps and text from an SRT file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    entries = []\n",
    "    srt_pattern = re.compile(r\"(\\d+)\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.*?)\\n\\n\", re.DOTALL)\n",
    "    \n",
    "    for match in srt_pattern.finditer(content):\n",
    "        index, start_time, end_time, text = match.groups()\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        entries.append({\"start_time\": start_time, \"end_time\": end_time, \"text\": text})\n",
    "    \n",
    "    return entries\n",
    "\n",
    "# to get ocr_regions\n",
    "def load_aws_ocr(file_path):\n",
    "    \"\"\"Loads AWS OCR results and extracts recognized text with bounding boxes and IDs.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    ocr_text_map = {\n",
    "        block[\"Id\"]: block.get(\"Text\", \"\") \n",
    "        for block in data.get(\"Blocks\", []) \n",
    "        if block.get(\"BlockType\") == \"LINE\"\n",
    "    }\n",
    "\n",
    "    regions = []\n",
    "    for block in data.get(\"Blocks\", []):\n",
    "        if block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\"):\n",
    "            child_blocks = [\n",
    "                child_id \n",
    "                for rel in block.get(\"Relationships\", []) \n",
    "                if rel[\"Type\"] == \"CHILD\" \n",
    "                for child_id in rel.get(\"Ids\", [])\n",
    "            ]\n",
    "            \n",
    "            # Check if any child block is also a LAYOUT_ type\n",
    "            if any(\n",
    "                child_block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\") \n",
    "                for child_block in data.get(\"Blocks\", []) \n",
    "                if child_block[\"Id\"] in child_blocks\n",
    "            ):\n",
    "                print(f\"Parent block {block['Id']} has layout children, hence skipped.\")\n",
    "                continue\n",
    "\n",
    "            text = [ocr_text_map[child_id] for child_id in child_blocks if child_id in ocr_text_map]\n",
    "            \n",
    "            regions.append({\n",
    "                \"Text\": \" \".join(text),\n",
    "                \"BoundingBox\": block[\"Geometry\"][\"BoundingBox\"],\n",
    "                \"BlockType\": block[\"BlockType\"],\n",
    "                \"Id\": block['Id']\n",
    "            })\n",
    "\n",
    "    return regions\n",
    "\n",
    "def extract_output_list(response):\n",
    "    \"\"\"Extracts only the list after 'Output List:' from the response.\"\"\"\n",
    "    match = re.search(r\"Output List:\\s*(\\[[^\\]]*\\])\", response)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Extract and return only the list\n",
    "    return \"[]\"  # Return an empty list if not found\n",
    "\n",
    "# for regions\n",
    "def generate_prompt(transcript_text, ocr_texts):\n",
    "    \"\"\"Creates a prompt for Qwen to find the best matching OCR regions.\"\"\"\n",
    "    ocr_texts_str = \"\\n\".join(ocr_texts)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Given the transcript: \\\"{transcript_text}\\\", find the best matching OCR regions from the list below. \"\n",
    "        \"Return only the most relevant ones, including figures if they contain relevant keywords.\\n\\n\"\n",
    "        f\"OCR Regions:\\n{ocr_texts_str}\\n\\n\"\n",
    "        \"Output format:\\n['Region: <Region Number>', ...]\\n\\n\"\n",
    "        \"Output List:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Additional function to check for figure relevance - just string matching \n",
    "def check_figure_relevance(transcript_text, figure_text):\n",
    "    \"\"\"Checks if a LAYOUT_FIGURE contains keywords related to the transcript.\"\"\"\n",
    "    transcript_words = set(transcript_text.lower().split())\n",
    "    figure_words = set(figure_text.lower().split())\n",
    "    \n",
    "    return any(word in figure_words for word in transcript_words)\n",
    "\n",
    "# Getting best matches\n",
    "def get_best_matches(transcript_text, ocr_regions):\n",
    "    \"\"\"Finds the best matching OCR regions for a transcript line using Qwen.\"\"\"\n",
    "    ocr_texts = [f\"Region {i+1}: {region['Text']} ({region['BlockType']})\" for i, region in enumerate(ocr_regions)]\n",
    "    ocr_texts_extract = {\n",
    "    f\"Region: {i+1}\": {\n",
    "        \"Text\": region[\"Text\"],\n",
    "        \"BlockType\": region[\"BlockType\"],\n",
    "        \"BoundingBox\": region[\"BoundingBox\"],\n",
    "        \"Id\": region['Id']\n",
    "    } \n",
    "    for i, region in enumerate(ocr_regions)\n",
    "}\n",
    "    prompt = generate_prompt(transcript_text, ocr_texts)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=2000)\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    output_list = extract_output_list(response)\n",
    "    output_list = ast.literal_eval(output_list)  # Convert to list\n",
    "    #print(f'this is tyoe of out list: {type(output_list)}')\n",
    "    '''for region in output_list:\n",
    "        print(f'This is region: {region}')'''\n",
    "           \n",
    "    matched_regions = [ocr_texts_extract[region] for region in output_list if region in ocr_texts_extract]\n",
    "    #print(f'this is matched regions: {matched_regions}')\n",
    "\n",
    "    return matched_regions\n",
    "\n",
    "# Slide matching\n",
    "def match_transcript_to_regions(srt_entries, ocr_regions):\n",
    "    \"\"\"Matches transcript lines to OCR regions using Qwen.\"\"\"\n",
    "    matched_results = []\n",
    "    for entry in srt_entries:\n",
    "        try:\n",
    "            matches = get_best_matches(entry[\"text\"], ocr_regions)\n",
    "            print(f'Checking matches: {matches}')\n",
    "            matched_results.append({\n",
    "                \"start_time\": entry[\"start_time\"],\n",
    "                \"end_time\": entry[\"end_time\"],\n",
    "                \"transcript\": entry[\"text\"],\n",
    "                \"MatchedRegion\": [\n",
    "                    match\n",
    "                    for match in matches if match[\"Text\"]  # Exclude empty text matches\n",
    "                ]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f'An error ocurred: {e}')\n",
    "    return matched_results\n",
    "\n",
    "\n",
    "# for getting entire ocr data\n",
    "def extract_ocr_text(json_data):\n",
    "    \"\"\"Extract full OCR text from AWS OCR JSON.\"\"\"\n",
    "    block_map = {block[\"Id\"]: block for block in json_data[\"Blocks\"]}\n",
    "    ocr_text = []\n",
    "    \n",
    "    for block in json_data[\"Blocks\"]:\n",
    "        if block[\"BlockType\"] == \"PAGE\" and \"Relationships\" in block:\n",
    "            for relation in block[\"Relationships\"]:\n",
    "                if relation[\"Type\"] == \"CHILD\":\n",
    "                    for child_id in relation[\"Ids\"]:\n",
    "                        child_block = block_map.get(child_id)\n",
    "                        if child_block and \"Text\" in child_block:\n",
    "                            ocr_text.append(child_block[\"Text\"])\n",
    "    \n",
    "    return \" \".join(ocr_text)\n",
    "\n",
    "\n",
    "# content overlap - to avoid animated slides (not used)\n",
    "def check_content_overlap(text1, text2, threshold=0.90):\n",
    "    \"\"\"Checks if at least `threshold` proportion of text1 is present in text2.\"\"\"\n",
    "    words1, words2 = text1.split(), text2.split()\n",
    "    if len(words1) == 0:\n",
    "        return False\n",
    "    match_count = sum(1 for word in words1 if word in words2)\n",
    "    return (match_count / len(words1)) >= threshold\n",
    "\n",
    "\n",
    "# To save results\n",
    "def save_results(matched_results, output_json):\n",
    "    \"\"\"Saves the matched results as JSON in the required format.\"\"\"\n",
    "    \n",
    "    formatted_results = []\n",
    "    \n",
    "    for result in matched_results:\n",
    "        formatted_entry = {\n",
    "            \"start_time\": result[\"start_time\"],\n",
    "            \"end_time\": result[\"end_time\"],\n",
    "            \"transcript\": result[\"transcript\"],\n",
    "            \"MatchedRegions\": []\n",
    "        }\n",
    "\n",
    "        for region in result.get(\"MatchedRegions\", []):  # Ensure it exists\n",
    "            #print(f'Inside loop: {region}')\n",
    "            #if region['Region'].get(\"Text\"):  # Only include meaningful text regions\n",
    "                #print('text exist')\n",
    "            formatted_entry[\"MatchedRegion\"].append(region)\n",
    "\n",
    "        formatted_results.append(formatted_entry)\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"MatchedRegions\": formatted_results}, file, indent=4)\n",
    "\n",
    "\n",
    "# Gather all transcript embeddings for full-slide similarity\n",
    "full_transcripts = {}\n",
    "for sub_dir in sorted(os.listdir(transcript_parent_dir)):  # Sorting for consistency\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    if os.path.isdir(sub_transcript_dir):\n",
    "        for file in sorted(os.listdir(sub_transcript_dir)):  # Sorting filenames\n",
    "            if file.endswith(\".srt\"):\n",
    "                file_path = os.path.join(sub_transcript_dir, file)\n",
    "                text = \" \".join([entry[\"text\"] for entry in parse_srt(file_path)])\n",
    "                full_transcripts[file] = {\"text\": text }\n",
    "\n",
    "\n",
    "# Iterate through slides for processing\n",
    "for sub_dir in sorted(os.listdir(image_parent_dir)):  # Sorting for consistency\n",
    "    sub_image_dir = os.path.join(image_parent_dir, sub_dir)\n",
    "    sub_transcript_dir = os.path.join(transcript_parent_dir, sub_dir)\n",
    "    sub_aws_ocr_dir = os.path.join(aws_ocr_parent_dir, sub_dir)\n",
    "    output_dir = os.path.join(output_parent_dir, sub_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not (os.path.isdir(sub_image_dir) and os.path.isdir(sub_transcript_dir) and os.path.isdir(sub_aws_ocr_dir)):\n",
    "        continue\n",
    "    \n",
    "    for file in sorted(os.listdir(sub_image_dir)):  # Sorting filenames\n",
    "        if file.endswith(\".png\"):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            file_path = os.path.join(output_parent_dir, sub_dir, f'{base_name}.json')\n",
    "            print(f'This is file path: {file_path}')\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'It is present: {file_path}')\n",
    "                continue\n",
    "            print(f'This is the basename: {base_name}')\n",
    "            image_path = os.path.join(sub_image_dir, file)\n",
    "            transcript_path = os.path.join(sub_transcript_dir, f\"{base_name}.srt\")\n",
    "            ocr_path = os.path.join(sub_aws_ocr_dir, f\"{base_name}.json\")\n",
    "            output_json = os.path.join(output_dir, f\"{base_name}.json\")\n",
    "            output_image = os.path.join(output_dir, f\"{base_name}_bbox.png\")\n",
    "            \n",
    "            if os.path.exists(transcript_path) and os.path.exists(ocr_path):\n",
    "                srt_entries = parse_srt(transcript_path)\n",
    "                ocr_regions = load_aws_ocr(ocr_path)\n",
    "                matched_results = match_transcript_to_regions(srt_entries, ocr_regions)\n",
    "                print(f'This is the matching regions: {matched_results}')\n",
    "                save_results(matched_results, output_json)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
