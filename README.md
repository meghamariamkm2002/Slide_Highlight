This repository contains the dataset and code for the paper:

Attend to What I Say: Highlighting Relevant Content on Slides

ðŸ“Œ Overview
The goal of this project is to automatically highlight slide content that is relevant to a given spoken narration, enhancing understanding and accessibility for viewers. This is particularly useful in educational and conference presentation settings where slides and speech are tightly coupled.

This repository includes:

A curated dataset of slide frames aligned with audio and transcripts 
Code for different methods

Dataset
The dataset consists of:
Slide frames extracted from conference talks
Speech transcripts (aligned to slides)
Ground-truth highlight annotations marking slide regions relevant to each spoken segment

Details:

Format: Images (PNG/JPG), Transcripts (SRT), Annotations (JSON), Audio (MP3)
Collected from various academic conferences (e.g., NeurIPS, ICML)

